{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzi+mvvOA1ahn+VmJp2QsA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ccd259848e2c42e6b609c92c534be628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a7445bd418444cba4fd1aa358e2611e",
              "IPY_MODEL_ae6f0a26f6434c22895f9a8d3287262c",
              "IPY_MODEL_d64fdf20a6d54b1197ee8a0e6fd47d2f"
            ],
            "layout": "IPY_MODEL_78c9c45d45ab45e88ce7ee1e08a836a9"
          }
        },
        "0a7445bd418444cba4fd1aa358e2611e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b169280d112d4530a8699d0b36d89be5",
            "placeholder": "​",
            "style": "IPY_MODEL_1b87a7f4706d4cb9a0d7dc3749eb4be8",
            "value": "100%"
          }
        },
        "ae6f0a26f6434c22895f9a8d3287262c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f175bdf38374b2d90611dde88449189",
            "max": 108949747,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_62a30c68f41048069e71b28d8219d50a",
            "value": 108949747
          }
        },
        "d64fdf20a6d54b1197ee8a0e6fd47d2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4f6134633ee47f5b64cdfe1575d6dd4",
            "placeholder": "​",
            "style": "IPY_MODEL_fd7ed8f437cd464eb9db51db478eebdf",
            "value": " 104M/104M [00:01&lt;00:00, 86.0MB/s]"
          }
        },
        "78c9c45d45ab45e88ce7ee1e08a836a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b169280d112d4530a8699d0b36d89be5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b87a7f4706d4cb9a0d7dc3749eb4be8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f175bdf38374b2d90611dde88449189": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62a30c68f41048069e71b28d8219d50a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e4f6134633ee47f5b64cdfe1575d6dd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd7ed8f437cd464eb9db51db478eebdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lohaoxi/basic-pytorch-gans/blob/master/01_vanilla_gan_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcHvlBr1zCAk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import math\n",
        "\n",
        "import scipy.linalg\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision.models import inception_v3\n",
        "\n",
        "if not os.path.exists('visuals'):\n",
        "    os.mkdir('visuals')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "K = 8\n",
        "N_EPOCHS = 512\n",
        "NOISE_DIM = 100\n",
        "IMAGE_DIM = 28*28\n",
        "MAXOUT_SIZE = 5\n",
        "HIDDEN_DIM = (1200, 625)"
      ],
      "metadata": {
        "id": "miLLXK9bliDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class FlattenTransform:\n",
        "    \n",
        "    def __call__(self, inputs):\n",
        "        return inputs.view(inputs.shape[0], -1)\n",
        "\n",
        "data_train = torchvision.datasets.MNIST(\n",
        "    \"./data/mnist\", \n",
        "    train=True, \n",
        "    download=True,\n",
        "    transform=torchvision.transforms.Compose([\n",
        "        torchvision.transforms.ToTensor(), \n",
        "        FlattenTransform()\n",
        "        ])\n",
        "    )\n",
        "\n",
        "loader_train = torch.utils.data.DataLoader(\n",
        "    data_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        "    )"
      ],
      "metadata": {
        "id": "kumOTwAh6PV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of observation: {0}\".format(len(data_train)))\n",
        "print(\"Size of each observation: {0}\".format(np.array(data_train[0][0]).shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jY6oq1uV7hlQ",
        "outputId": "a7831cbe-e3ee-4c3a-d4e7-c0fd72f35b1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of observation: 60000\n",
            "Size of each observation: (1, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class Maxout(nn.Module):\n",
        "\n",
        "    def __init__(self, n_pieces):\n",
        "        super(Maxout, self).__init__()\n",
        "        self.n_pieces = n_pieces\n",
        "        \n",
        "    def forward(self, batch):\n",
        "        assert batch.shape[1] % self.n_pieces == 0\n",
        "        batch = batch.view(\n",
        "            batch.shape[0], \n",
        "            batch.shape[1] // self.n_pieces, \n",
        "            self.n_pieces\n",
        "            )\n",
        "        batch, _ = batch.max(dim=2)\n",
        "        return batch\n",
        "    \n",
        "\n",
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, noise_dim, hid_dim, out_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.noise_dim = noise_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.out_dim = out_dim\n",
        "\n",
        "        self.fc1 = nn.Linear(self.noise_dim, self.hid_dim, bias=True)\n",
        "        self.fc2 = nn.Linear(self.hid_dim, self.hid_dim, bias=True)\n",
        "        self.fc3 = nn.Linear(self.hid_dim, self.hid_dim, bias=True)\n",
        "        self.fc4 = nn.Linear(self.hid_dim, self.out_dim, bias=True)\n",
        "\n",
        "    def forward(self, batch):\n",
        "\n",
        "        batch = F.dropout(F.relu(self.fc1(batch)))\n",
        "        batch = F.dropout(F.relu(self.fc2(batch)))\n",
        "        batch = F.dropout(F.relu(self.fc3(batch)))\n",
        "        batch = torch.sigmoid(self.fc4(batch))\n",
        "\n",
        "        return batch\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, hid_dim, out_dim, maxout_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.maxout_size = maxout_size\n",
        "        self.Maxout = Maxout(maxout_size)\n",
        "\n",
        "        self.fc1 = nn.Linear(self.in_dim, self.hid_dim, bias=True)\n",
        "        self.fc2 = nn.Linear(self.hid_dim // self.maxout_size, self.hid_dim, bias=True)\n",
        "        self.fc3 = nn.Linear(self.hid_dim // self.maxout_size, self.out_dim, bias=True)\n",
        "\n",
        "    def forward(self, batch):\n",
        "        batch = batch.view(batch.size(0), -1)\n",
        "        batch = self.Maxout(self.fc1(batch))\n",
        "        batch = self.Maxout(self.fc2(batch))\n",
        "        batch = torch.sigmoid(self.fc3(batch))\n",
        "        \n",
        "        return batch"
      ],
      "metadata": {
        "id": "6MVk_7s_7sxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Generator(NOISE_DIM, HIDDEN_DIM[0], IMAGE_DIM).to(device)\n",
        "discriminator = Discriminator(IMAGE_DIM, HIDDEN_DIM[1], 1, MAXOUT_SIZE).to(device)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"Number of parameters in the Generatoris: {}\".format(count_parameters(generator)))\n",
        "print(\"Number of parameters in the Discriminator: {}\".format(count_parameters(discriminator)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9idKsjMG7vb_",
        "outputId": "39b75895-4e42-4152-b9a7-bb18a81e749e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters in the Generatoris: 3945184\n",
            "Number of parameters in the Discriminator: 569501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "real_labels = torch.ones(BATCH_SIZE, 1).to(device)\n",
        "fake_labels = torch.zeros(BATCH_SIZE, 1).to(device)\n",
        "\n",
        "test_set = torch.randn(16, NOISE_DIM).to(device)\n",
        "\n",
        "num_steps = len(loader_train) // BATCH_SIZE\n",
        "\n",
        "discriminator_optimizer = torch.optim.SGD(\n",
        "    discriminator.parameters(),\n",
        "    lr=0.0002,\n",
        "    momentum=0.5\n",
        ")\n",
        "\n",
        "generator_optimizer = torch.optim.SGD(\n",
        "    generator.parameters(),\n",
        "    lr=0.0002,\n",
        "    momentum=0.5\n",
        ")\n",
        "\n",
        "criterion = torch.nn.BCELoss()"
      ],
      "metadata": {
        "id": "WKl7ZKSW-ylP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "   \n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    D_loss = 0\n",
        "    G_loss = 0\n",
        "    D_x = 0\n",
        "    D_g_z1 = 0\n",
        "    D_g_z2 = 0\n",
        "\n",
        "    for i, (images, _) in enumerate(loader_train):\n",
        "\n",
        "        if i == num_steps: break\n",
        "\n",
        "        # Train Discriminator\n",
        "\n",
        "        for _ in range(K):\n",
        "            \n",
        "            real_images = images.to(device) # [batch_size, 784]\n",
        "\n",
        "            fake_images = generator(torch.randn(BATCH_SIZE, NOISE_DIM).to(device)) # [batch_size, 784]\n",
        "\n",
        "            discriminator_optimizer.zero_grad()\n",
        "\n",
        "            real_outputs = discriminator(real_images)\n",
        "            fake_outputs = discriminator(fake_images)\n",
        "\n",
        "            discriminator_err_real = criterion(real_outputs, real_labels)\n",
        "            discriminator_err_fake = criterion(fake_outputs, fake_labels)\n",
        "\n",
        "            discriminator_err_real.backward()\n",
        "            discriminator_err_fake.backward()\n",
        "\n",
        "            discriminator_optimizer.step()\n",
        "\n",
        "        # Train Generator\n",
        "        z = torch.randn(BATCH_SIZE, NOISE_DIM).to(device)\n",
        "\n",
        "        generator.zero_grad()\n",
        "\n",
        "        outputs = discriminator(generator(z))\n",
        "\n",
        "        generator_err = criterion(outputs, real_labels)\n",
        "\n",
        "        generator_err.backward()\n",
        "\n",
        "        generator_optimizer.step()\n",
        "\n",
        "        # Loss calculation\n",
        "        D_loss += (discriminator_err_real + discriminator_err_fake).item()\n",
        "        G_loss += generator_err.item()\n",
        "        D_x += real_outputs.mean().item()\n",
        "        D_g_z1 += fake_outputs.mean().item()\n",
        "        D_g_z2 += outputs.mean().item()\n",
        "\n",
        "    print(\"epcoh: {}\\t D_loss: {:.4f}\\t G_loss: {:.4f}\\t D_x: {:.4f}\\t D_g_z1: {:.4f}\\t D_g_z2: {:.4f}\\t\".format(\n",
        "        str(epoch).zfill(6), \n",
        "        D_loss / num_steps / K,\n",
        "        G_loss / num_steps,\n",
        "        D_x / num_steps / K,\n",
        "        D_g_z1 / num_steps / K,\n",
        "        D_g_z2 / num_steps\n",
        "    )\n",
        "    )\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "\n",
        "        generated = generator(test_set).detach().cpu().view(-1, 1, 28, 28)\n",
        "\n",
        "        grid = torchvision.utils.save_image(\n",
        "            generated,\n",
        "            os.path.join(\n",
        "                'visuals',\n",
        "                '{}.jpg'.format(\n",
        "                    str(epoch).zfill(6)\n",
        "                )\n",
        "            ),\n",
        "            nrow=4,\n",
        "            padding=10,\n",
        "            pad_value=1\n",
        "        )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLaIiWN5jFzn",
        "outputId": "8a367487-d268-4db8-d97a-e71e938dd562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epcoh: 000000\t D_loss: 0.1377\t G_loss: 1.2100\t D_x: 0.0602\t D_g_z1: 0.0382\t D_g_z2: 0.3052\t\n",
            "epcoh: 000001\t D_loss: 0.1010\t G_loss: 1.7840\t D_x: 0.0672\t D_g_z1: 0.0212\t D_g_z2: 0.1694\t\n",
            "epcoh: 000002\t D_loss: 0.0703\t G_loss: 2.2327\t D_x: 0.0800\t D_g_z1: 0.0135\t D_g_z2: 0.1082\t\n",
            "epcoh: 000003\t D_loss: 0.0452\t G_loss: 2.6831\t D_x: 0.0937\t D_g_z1: 0.0086\t D_g_z2: 0.0689\t\n",
            "epcoh: 000004\t D_loss: 0.0286\t G_loss: 3.1291\t D_x: 0.1042\t D_g_z1: 0.0055\t D_g_z2: 0.0441\t\n",
            "epcoh: 000005\t D_loss: 0.0191\t G_loss: 3.5366\t D_x: 0.1107\t D_g_z1: 0.0037\t D_g_z2: 0.0293\t\n",
            "epcoh: 000006\t D_loss: 0.0131\t G_loss: 3.8869\t D_x: 0.1150\t D_g_z1: 0.0026\t D_g_z2: 0.0206\t\n",
            "epcoh: 000007\t D_loss: 0.0098\t G_loss: 4.1848\t D_x: 0.1175\t D_g_z1: 0.0019\t D_g_z2: 0.0153\t\n",
            "epcoh: 000008\t D_loss: 0.0076\t G_loss: 4.4388\t D_x: 0.1191\t D_g_z1: 0.0015\t D_g_z2: 0.0118\t\n",
            "epcoh: 000009\t D_loss: 0.0061\t G_loss: 4.6561\t D_x: 0.1202\t D_g_z1: 0.0012\t D_g_z2: 0.0095\t\n",
            "epcoh: 000010\t D_loss: 0.0049\t G_loss: 4.8480\t D_x: 0.1211\t D_g_z1: 0.0010\t D_g_z2: 0.0079\t\n",
            "epcoh: 000011\t D_loss: 0.0043\t G_loss: 5.0123\t D_x: 0.1216\t D_g_z1: 0.0008\t D_g_z2: 0.0067\t\n",
            "epcoh: 000012\t D_loss: 0.0037\t G_loss: 5.1573\t D_x: 0.1221\t D_g_z1: 0.0007\t D_g_z2: 0.0058\t\n",
            "epcoh: 000013\t D_loss: 0.0031\t G_loss: 5.2936\t D_x: 0.1225\t D_g_z1: 0.0006\t D_g_z2: 0.0050\t\n",
            "epcoh: 000014\t D_loss: 0.0028\t G_loss: 5.4081\t D_x: 0.1228\t D_g_z1: 0.0006\t D_g_z2: 0.0045\t\n",
            "epcoh: 000015\t D_loss: 0.0026\t G_loss: 5.5126\t D_x: 0.1230\t D_g_z1: 0.0005\t D_g_z2: 0.0040\t\n",
            "epcoh: 000016\t D_loss: 0.0024\t G_loss: 5.5992\t D_x: 0.1231\t D_g_z1: 0.0005\t D_g_z2: 0.0037\t\n",
            "epcoh: 000017\t D_loss: 0.0021\t G_loss: 5.6822\t D_x: 0.1234\t D_g_z1: 0.0004\t D_g_z2: 0.0034\t\n",
            "epcoh: 000018\t D_loss: 0.0020\t G_loss: 5.7591\t D_x: 0.1235\t D_g_z1: 0.0004\t D_g_z2: 0.0032\t\n",
            "epcoh: 000019\t D_loss: 0.0018\t G_loss: 5.8196\t D_x: 0.1236\t D_g_z1: 0.0004\t D_g_z2: 0.0030\t\n",
            "epcoh: 000020\t D_loss: 0.0017\t G_loss: 5.8826\t D_x: 0.1237\t D_g_z1: 0.0003\t D_g_z2: 0.0028\t\n",
            "epcoh: 000021\t D_loss: 0.0015\t G_loss: 5.9344\t D_x: 0.1238\t D_g_z1: 0.0003\t D_g_z2: 0.0027\t\n",
            "epcoh: 000022\t D_loss: 0.0015\t G_loss: 5.9629\t D_x: 0.1239\t D_g_z1: 0.0003\t D_g_z2: 0.0026\t\n",
            "epcoh: 000023\t D_loss: 0.0014\t G_loss: 5.9870\t D_x: 0.1239\t D_g_z1: 0.0003\t D_g_z2: 0.0025\t\n",
            "epcoh: 000024\t D_loss: 0.0014\t G_loss: 6.0018\t D_x: 0.1240\t D_g_z1: 0.0003\t D_g_z2: 0.0025\t\n",
            "epcoh: 000025\t D_loss: 0.0013\t G_loss: 5.9725\t D_x: 0.1240\t D_g_z1: 0.0003\t D_g_z2: 0.0026\t\n",
            "epcoh: 000026\t D_loss: 0.0014\t G_loss: 5.9318\t D_x: 0.1240\t D_g_z1: 0.0003\t D_g_z2: 0.0027\t\n",
            "epcoh: 000027\t D_loss: 0.0013\t G_loss: 5.8578\t D_x: 0.1241\t D_g_z1: 0.0004\t D_g_z2: 0.0030\t\n",
            "epcoh: 000028\t D_loss: 0.0014\t G_loss: 5.7685\t D_x: 0.1241\t D_g_z1: 0.0004\t D_g_z2: 0.0033\t\n",
            "epcoh: 000029\t D_loss: 0.0014\t G_loss: 5.6154\t D_x: 0.1241\t D_g_z1: 0.0005\t D_g_z2: 0.0040\t\n",
            "epcoh: 000030\t D_loss: 0.0016\t G_loss: 5.4190\t D_x: 0.1241\t D_g_z1: 0.0006\t D_g_z2: 0.0050\t\n",
            "epcoh: 000031\t D_loss: 0.0019\t G_loss: 5.1960\t D_x: 0.1240\t D_g_z1: 0.0009\t D_g_z2: 0.0068\t\n",
            "epcoh: 000032\t D_loss: 0.0025\t G_loss: 4.9551\t D_x: 0.1238\t D_g_z1: 0.0012\t D_g_z2: 0.0097\t\n",
            "epcoh: 000033\t D_loss: 0.0033\t G_loss: 4.7035\t D_x: 0.1235\t D_g_z1: 0.0017\t D_g_z2: 0.0146\t\n",
            "epcoh: 000034\t D_loss: 0.0048\t G_loss: 4.6242\t D_x: 0.1227\t D_g_z1: 0.0024\t D_g_z2: 0.0186\t\n",
            "epcoh: 000035\t D_loss: 0.0073\t G_loss: 4.4241\t D_x: 0.1211\t D_g_z1: 0.0031\t D_g_z2: 0.0261\t\n",
            "epcoh: 000036\t D_loss: 0.0110\t G_loss: 4.2684\t D_x: 0.1188\t D_g_z1: 0.0040\t D_g_z2: 0.0314\t\n",
            "epcoh: 000037\t D_loss: 0.0156\t G_loss: 3.9905\t D_x: 0.1159\t D_g_z1: 0.0052\t D_g_z2: 0.0395\t\n",
            "epcoh: 000038\t D_loss: 0.0199\t G_loss: 3.6706\t D_x: 0.1134\t D_g_z1: 0.0064\t D_g_z2: 0.0487\t\n",
            "epcoh: 000039\t D_loss: 0.0215\t G_loss: 3.3884\t D_x: 0.1122\t D_g_z1: 0.0067\t D_g_z2: 0.0558\t\n",
            "epcoh: 000040\t D_loss: 0.0224\t G_loss: 3.2765\t D_x: 0.1117\t D_g_z1: 0.0072\t D_g_z2: 0.0568\t\n",
            "epcoh: 000041\t D_loss: 0.0232\t G_loss: 3.1283\t D_x: 0.1112\t D_g_z1: 0.0075\t D_g_z2: 0.0595\t\n",
            "epcoh: 000042\t D_loss: 0.0236\t G_loss: 3.0118\t D_x: 0.1111\t D_g_z1: 0.0077\t D_g_z2: 0.0637\t\n",
            "epcoh: 000043\t D_loss: 0.0226\t G_loss: 2.9744\t D_x: 0.1115\t D_g_z1: 0.0075\t D_g_z2: 0.0618\t\n",
            "epcoh: 000044\t D_loss: 0.0209\t G_loss: 2.9910\t D_x: 0.1125\t D_g_z1: 0.0072\t D_g_z2: 0.0581\t\n",
            "epcoh: 000045\t D_loss: 0.0215\t G_loss: 2.9319\t D_x: 0.1124\t D_g_z1: 0.0075\t D_g_z2: 0.0595\t\n",
            "epcoh: 000046\t D_loss: 0.0227\t G_loss: 2.8222\t D_x: 0.1120\t D_g_z1: 0.0082\t D_g_z2: 0.0655\t\n",
            "epcoh: 000047\t D_loss: 0.0226\t G_loss: 2.8031\t D_x: 0.1122\t D_g_z1: 0.0082\t D_g_z2: 0.0660\t\n",
            "epcoh: 000048\t D_loss: 0.0211\t G_loss: 2.8163\t D_x: 0.1133\t D_g_z1: 0.0079\t D_g_z2: 0.0641\t\n",
            "epcoh: 000049\t D_loss: 0.0185\t G_loss: 2.9274\t D_x: 0.1149\t D_g_z1: 0.0071\t D_g_z2: 0.0566\t\n",
            "epcoh: 000050\t D_loss: 0.0168\t G_loss: 2.9721\t D_x: 0.1159\t D_g_z1: 0.0067\t D_g_z2: 0.0540\t\n",
            "epcoh: 000051\t D_loss: 0.0164\t G_loss: 2.9867\t D_x: 0.1164\t D_g_z1: 0.0066\t D_g_z2: 0.0531\t\n",
            "epcoh: 000052\t D_loss: 0.0147\t G_loss: 3.0557\t D_x: 0.1174\t D_g_z1: 0.0061\t D_g_z2: 0.0492\t\n",
            "epcoh: 000053\t D_loss: 0.0150\t G_loss: 3.0231\t D_x: 0.1173\t D_g_z1: 0.0064\t D_g_z2: 0.0508\t\n",
            "epcoh: 000054\t D_loss: 0.0138\t G_loss: 3.0687\t D_x: 0.1181\t D_g_z1: 0.0060\t D_g_z2: 0.0484\t\n",
            "epcoh: 000055\t D_loss: 0.0139\t G_loss: 3.0317\t D_x: 0.1182\t D_g_z1: 0.0062\t D_g_z2: 0.0505\t\n",
            "epcoh: 000056\t D_loss: 0.0144\t G_loss: 2.9936\t D_x: 0.1183\t D_g_z1: 0.0065\t D_g_z2: 0.0521\t\n",
            "epcoh: 000057\t D_loss: 0.0164\t G_loss: 2.8394\t D_x: 0.1176\t D_g_z1: 0.0076\t D_g_z2: 0.0614\t\n",
            "epcoh: 000058\t D_loss: 0.0168\t G_loss: 2.7981\t D_x: 0.1176\t D_g_z1: 0.0081\t D_g_z2: 0.0638\t\n",
            "epcoh: 000059\t D_loss: 0.0210\t G_loss: 2.5484\t D_x: 0.1164\t D_g_z1: 0.0104\t D_g_z2: 0.0830\t\n",
            "epcoh: 000060\t D_loss: 0.0202\t G_loss: 2.5432\t D_x: 0.1172\t D_g_z1: 0.0105\t D_g_z2: 0.0828\t\n",
            "epcoh: 000061\t D_loss: 0.0215\t G_loss: 2.4518\t D_x: 0.1175\t D_g_z1: 0.0117\t D_g_z2: 0.0914\t\n",
            "epcoh: 000062\t D_loss: 0.0206\t G_loss: 2.4348\t D_x: 0.1181\t D_g_z1: 0.0117\t D_g_z2: 0.0930\t\n",
            "epcoh: 000063\t D_loss: 0.0199\t G_loss: 2.4427\t D_x: 0.1185\t D_g_z1: 0.0117\t D_g_z2: 0.0922\t\n",
            "epcoh: 000064\t D_loss: 0.0303\t G_loss: 2.0498\t D_x: 0.1164\t D_g_z1: 0.0175\t D_g_z2: 0.1377\t\n",
            "epcoh: 000065\t D_loss: 0.0420\t G_loss: 1.7321\t D_x: 0.1143\t D_g_z1: 0.0247\t D_g_z2: 0.1913\t\n",
            "epcoh: 000066\t D_loss: 0.0494\t G_loss: 1.5257\t D_x: 0.1148\t D_g_z1: 0.0302\t D_g_z2: 0.2386\t\n",
            "epcoh: 000067\t D_loss: 0.0722\t G_loss: 1.1707\t D_x: 0.1110\t D_g_z1: 0.0419\t D_g_z2: 0.3300\t\n",
            "epcoh: 000068\t D_loss: 0.0503\t G_loss: 1.4351\t D_x: 0.1147\t D_g_z1: 0.0317\t D_g_z2: 0.2514\t\n",
            "epcoh: 000069\t D_loss: 0.0481\t G_loss: 1.4814\t D_x: 0.1148\t D_g_z1: 0.0305\t D_g_z2: 0.2408\t\n",
            "epcoh: 000070\t D_loss: 0.0348\t G_loss: 1.7701\t D_x: 0.1172\t D_g_z1: 0.0229\t D_g_z2: 0.1801\t\n",
            "epcoh: 000071\t D_loss: 0.0300\t G_loss: 1.9260\t D_x: 0.1174\t D_g_z1: 0.0193\t D_g_z2: 0.1565\t\n",
            "epcoh: 000072\t D_loss: 0.0231\t G_loss: 2.2118\t D_x: 0.1183\t D_g_z1: 0.0144\t D_g_z2: 0.1158\t\n",
            "epcoh: 000073\t D_loss: 0.0191\t G_loss: 2.4427\t D_x: 0.1187\t D_g_z1: 0.0113\t D_g_z2: 0.0923\t\n",
            "epcoh: 000074\t D_loss: 0.0197\t G_loss: 2.4818\t D_x: 0.1181\t D_g_z1: 0.0111\t D_g_z2: 0.0886\t\n",
            "epcoh: 000075\t D_loss: 0.0165\t G_loss: 2.6843\t D_x: 0.1189\t D_g_z1: 0.0089\t D_g_z2: 0.0716\t\n",
            "epcoh: 000076\t D_loss: 0.0135\t G_loss: 2.9049\t D_x: 0.1198\t D_g_z1: 0.0072\t D_g_z2: 0.0579\t\n",
            "epcoh: 000077\t D_loss: 0.0123\t G_loss: 3.0611\t D_x: 0.1198\t D_g_z1: 0.0062\t D_g_z2: 0.0500\t\n",
            "epcoh: 000078\t D_loss: 0.0115\t G_loss: 3.1584\t D_x: 0.1203\t D_g_z1: 0.0059\t D_g_z2: 0.0461\t\n",
            "epcoh: 000079\t D_loss: 0.0100\t G_loss: 3.3093\t D_x: 0.1209\t D_g_z1: 0.0050\t D_g_z2: 0.0401\t\n",
            "epcoh: 000080\t D_loss: 0.0110\t G_loss: 3.2364\t D_x: 0.1207\t D_g_z1: 0.0054\t D_g_z2: 0.0430\t\n",
            "epcoh: 000081\t D_loss: 0.0099\t G_loss: 3.3513\t D_x: 0.1210\t D_g_z1: 0.0048\t D_g_z2: 0.0381\t\n",
            "epcoh: 000082\t D_loss: 0.0097\t G_loss: 3.4113\t D_x: 0.1209\t D_g_z1: 0.0047\t D_g_z2: 0.0369\t\n",
            "epcoh: 000083\t D_loss: 0.0093\t G_loss: 3.4271\t D_x: 0.1211\t D_g_z1: 0.0046\t D_g_z2: 0.0363\t\n",
            "epcoh: 000084\t D_loss: 0.0100\t G_loss: 3.4351\t D_x: 0.1208\t D_g_z1: 0.0047\t D_g_z2: 0.0362\t\n",
            "epcoh: 000085\t D_loss: 0.0083\t G_loss: 3.5258\t D_x: 0.1215\t D_g_z1: 0.0041\t D_g_z2: 0.0324\t\n",
            "epcoh: 000086\t D_loss: 0.0089\t G_loss: 3.4807\t D_x: 0.1212\t D_g_z1: 0.0042\t D_g_z2: 0.0336\t\n",
            "epcoh: 000087\t D_loss: 0.0091\t G_loss: 3.5360\t D_x: 0.1209\t D_g_z1: 0.0039\t D_g_z2: 0.0317\t\n",
            "epcoh: 000088\t D_loss: 0.0083\t G_loss: 3.6118\t D_x: 0.1214\t D_g_z1: 0.0038\t D_g_z2: 0.0300\t\n",
            "epcoh: 000089\t D_loss: 0.0079\t G_loss: 3.6644\t D_x: 0.1215\t D_g_z1: 0.0035\t D_g_z2: 0.0284\t\n",
            "epcoh: 000090\t D_loss: 0.0069\t G_loss: 3.7349\t D_x: 0.1222\t D_g_z1: 0.0034\t D_g_z2: 0.0258\t\n",
            "epcoh: 000091\t D_loss: 0.0075\t G_loss: 3.6028\t D_x: 0.1222\t D_g_z1: 0.0039\t D_g_z2: 0.0305\t\n",
            "epcoh: 000092\t D_loss: 0.0078\t G_loss: 3.5747\t D_x: 0.1219\t D_g_z1: 0.0041\t D_g_z2: 0.0318\t\n",
            "epcoh: 000093\t D_loss: 0.0068\t G_loss: 3.7189\t D_x: 0.1222\t D_g_z1: 0.0035\t D_g_z2: 0.0278\t\n",
            "epcoh: 000094\t D_loss: 0.0052\t G_loss: 3.9664\t D_x: 0.1227\t D_g_z1: 0.0026\t D_g_z2: 0.0207\t\n",
            "epcoh: 000095\t D_loss: 0.0060\t G_loss: 3.8312\t D_x: 0.1225\t D_g_z1: 0.0031\t D_g_z2: 0.0245\t\n",
            "epcoh: 000096\t D_loss: 0.0058\t G_loss: 3.8905\t D_x: 0.1224\t D_g_z1: 0.0030\t D_g_z2: 0.0228\t\n",
            "epcoh: 000097\t D_loss: 0.0065\t G_loss: 3.8186\t D_x: 0.1221\t D_g_z1: 0.0030\t D_g_z2: 0.0241\t\n",
            "epcoh: 000098\t D_loss: 0.0080\t G_loss: 3.7109\t D_x: 0.1215\t D_g_z1: 0.0035\t D_g_z2: 0.0271\t\n",
            "epcoh: 000099\t D_loss: 0.0056\t G_loss: 4.0200\t D_x: 0.1225\t D_g_z1: 0.0024\t D_g_z2: 0.0194\t\n",
            "epcoh: 000100\t D_loss: 0.0055\t G_loss: 3.9504\t D_x: 0.1227\t D_g_z1: 0.0027\t D_g_z2: 0.0215\t\n",
            "epcoh: 000101\t D_loss: 0.0061\t G_loss: 3.9369\t D_x: 0.1224\t D_g_z1: 0.0029\t D_g_z2: 0.0222\t\n",
            "epcoh: 000102\t D_loss: 0.0055\t G_loss: 3.9848\t D_x: 0.1226\t D_g_z1: 0.0027\t D_g_z2: 0.0217\t\n",
            "epcoh: 000103\t D_loss: 0.0070\t G_loss: 3.7649\t D_x: 0.1221\t D_g_z1: 0.0034\t D_g_z2: 0.0266\t\n",
            "epcoh: 000104\t D_loss: 0.0063\t G_loss: 3.8072\t D_x: 0.1224\t D_g_z1: 0.0032\t D_g_z2: 0.0252\t\n",
            "epcoh: 000105\t D_loss: 0.0054\t G_loss: 3.9445\t D_x: 0.1228\t D_g_z1: 0.0028\t D_g_z2: 0.0215\t\n",
            "epcoh: 000106\t D_loss: 0.0044\t G_loss: 4.2182\t D_x: 0.1231\t D_g_z1: 0.0021\t D_g_z2: 0.0162\t\n",
            "epcoh: 000107\t D_loss: 0.0043\t G_loss: 4.2406\t D_x: 0.1231\t D_g_z1: 0.0020\t D_g_z2: 0.0155\t\n",
            "epcoh: 000108\t D_loss: 0.0041\t G_loss: 4.1704\t D_x: 0.1233\t D_g_z1: 0.0021\t D_g_z2: 0.0167\t\n",
            "epcoh: 000109\t D_loss: 0.0048\t G_loss: 4.0699\t D_x: 0.1230\t D_g_z1: 0.0024\t D_g_z2: 0.0187\t\n",
            "epcoh: 000110\t D_loss: 0.0044\t G_loss: 4.0781\t D_x: 0.1232\t D_g_z1: 0.0022\t D_g_z2: 0.0186\t\n",
            "epcoh: 000111\t D_loss: 0.0071\t G_loss: 3.7770\t D_x: 0.1221\t D_g_z1: 0.0032\t D_g_z2: 0.0256\t\n",
            "epcoh: 000112\t D_loss: 0.0061\t G_loss: 3.8930\t D_x: 0.1225\t D_g_z1: 0.0028\t D_g_z2: 0.0224\t\n",
            "epcoh: 000113\t D_loss: 0.0071\t G_loss: 3.8371\t D_x: 0.1222\t D_g_z1: 0.0031\t D_g_z2: 0.0241\t\n",
            "epcoh: 000114\t D_loss: 0.0083\t G_loss: 3.7049\t D_x: 0.1216\t D_g_z1: 0.0034\t D_g_z2: 0.0271\t\n",
            "epcoh: 000115\t D_loss: 0.0056\t G_loss: 4.0424\t D_x: 0.1226\t D_g_z1: 0.0024\t D_g_z2: 0.0194\t\n",
            "epcoh: 000116\t D_loss: 0.0070\t G_loss: 3.8579\t D_x: 0.1222\t D_g_z1: 0.0032\t D_g_z2: 0.0250\t\n",
            "epcoh: 000117\t D_loss: 0.0068\t G_loss: 3.8611\t D_x: 0.1223\t D_g_z1: 0.0030\t D_g_z2: 0.0237\t\n",
            "epcoh: 000118\t D_loss: 0.0063\t G_loss: 3.8685\t D_x: 0.1225\t D_g_z1: 0.0030\t D_g_z2: 0.0241\t\n",
            "epcoh: 000119\t D_loss: 0.0069\t G_loss: 3.6862\t D_x: 0.1224\t D_g_z1: 0.0035\t D_g_z2: 0.0288\t\n",
            "epcoh: 000120\t D_loss: 0.0061\t G_loss: 3.7721\t D_x: 0.1226\t D_g_z1: 0.0032\t D_g_z2: 0.0255\t\n",
            "epcoh: 000121\t D_loss: 0.0064\t G_loss: 3.7433\t D_x: 0.1227\t D_g_z1: 0.0035\t D_g_z2: 0.0273\t\n",
            "epcoh: 000122\t D_loss: 0.0091\t G_loss: 3.5585\t D_x: 0.1215\t D_g_z1: 0.0043\t D_g_z2: 0.0332\t\n",
            "epcoh: 000123\t D_loss: 0.0066\t G_loss: 3.8479\t D_x: 0.1223\t D_g_z1: 0.0031\t D_g_z2: 0.0248\t\n",
            "epcoh: 000124\t D_loss: 0.0064\t G_loss: 3.9283\t D_x: 0.1222\t D_g_z1: 0.0029\t D_g_z2: 0.0222\t\n",
            "epcoh: 000125\t D_loss: 0.0059\t G_loss: 4.0463\t D_x: 0.1224\t D_g_z1: 0.0024\t D_g_z2: 0.0198\t\n",
            "epcoh: 000126\t D_loss: 0.0060\t G_loss: 4.0201\t D_x: 0.1224\t D_g_z1: 0.0026\t D_g_z2: 0.0202\t\n",
            "epcoh: 000127\t D_loss: 0.0059\t G_loss: 4.0258\t D_x: 0.1227\t D_g_z1: 0.0027\t D_g_z2: 0.0206\t\n",
            "epcoh: 000128\t D_loss: 0.0059\t G_loss: 3.9956\t D_x: 0.1223\t D_g_z1: 0.0025\t D_g_z2: 0.0205\t\n",
            "epcoh: 000129\t D_loss: 0.0047\t G_loss: 4.2602\t D_x: 0.1228\t D_g_z1: 0.0020\t D_g_z2: 0.0156\t\n",
            "epcoh: 000130\t D_loss: 0.0055\t G_loss: 4.1508\t D_x: 0.1225\t D_g_z1: 0.0022\t D_g_z2: 0.0173\t\n",
            "epcoh: 000131\t D_loss: 0.0047\t G_loss: 4.2712\t D_x: 0.1228\t D_g_z1: 0.0019\t D_g_z2: 0.0153\t\n",
            "epcoh: 000132\t D_loss: 0.0060\t G_loss: 4.0818\t D_x: 0.1224\t D_g_z1: 0.0025\t D_g_z2: 0.0191\t\n",
            "epcoh: 000133\t D_loss: 0.0054\t G_loss: 4.2085\t D_x: 0.1225\t D_g_z1: 0.0021\t D_g_z2: 0.0166\t\n",
            "epcoh: 000134\t D_loss: 0.0057\t G_loss: 4.0446\t D_x: 0.1226\t D_g_z1: 0.0025\t D_g_z2: 0.0197\t\n",
            "epcoh: 000135\t D_loss: 0.0046\t G_loss: 4.3191\t D_x: 0.1229\t D_g_z1: 0.0018\t D_g_z2: 0.0146\t\n",
            "epcoh: 000136\t D_loss: 0.0034\t G_loss: 4.5050\t D_x: 0.1236\t D_g_z1: 0.0016\t D_g_z2: 0.0126\t\n",
            "epcoh: 000137\t D_loss: 0.0047\t G_loss: 4.1555\t D_x: 0.1231\t D_g_z1: 0.0024\t D_g_z2: 0.0190\t\n",
            "epcoh: 000138\t D_loss: 0.0063\t G_loss: 3.9929\t D_x: 0.1224\t D_g_z1: 0.0029\t D_g_z2: 0.0219\t\n",
            "epcoh: 000139\t D_loss: 0.0047\t G_loss: 4.1662\t D_x: 0.1230\t D_g_z1: 0.0022\t D_g_z2: 0.0179\t\n",
            "epcoh: 000140\t D_loss: 0.0050\t G_loss: 4.1052\t D_x: 0.1231\t D_g_z1: 0.0025\t D_g_z2: 0.0189\t\n",
            "epcoh: 000141\t D_loss: 0.0041\t G_loss: 4.2818\t D_x: 0.1234\t D_g_z1: 0.0020\t D_g_z2: 0.0162\t\n",
            "epcoh: 000142\t D_loss: 0.0039\t G_loss: 4.2212\t D_x: 0.1236\t D_g_z1: 0.0021\t D_g_z2: 0.0167\t\n",
            "epcoh: 000143\t D_loss: 0.0030\t G_loss: 4.5276\t D_x: 0.1239\t D_g_z1: 0.0016\t D_g_z2: 0.0128\t\n",
            "epcoh: 000144\t D_loss: 0.0041\t G_loss: 4.2072\t D_x: 0.1234\t D_g_z1: 0.0022\t D_g_z2: 0.0172\t\n",
            "epcoh: 000145\t D_loss: 0.0042\t G_loss: 4.1579\t D_x: 0.1234\t D_g_z1: 0.0022\t D_g_z2: 0.0172\t\n",
            "epcoh: 000146\t D_loss: 0.0039\t G_loss: 4.2235\t D_x: 0.1234\t D_g_z1: 0.0020\t D_g_z2: 0.0162\t\n",
            "epcoh: 000147\t D_loss: 0.0041\t G_loss: 4.2842\t D_x: 0.1234\t D_g_z1: 0.0020\t D_g_z2: 0.0158\t\n",
            "epcoh: 000148\t D_loss: 0.0046\t G_loss: 4.1586\t D_x: 0.1231\t D_g_z1: 0.0023\t D_g_z2: 0.0179\t\n",
            "epcoh: 000149\t D_loss: 0.0037\t G_loss: 4.4299\t D_x: 0.1235\t D_g_z1: 0.0017\t D_g_z2: 0.0136\t\n",
            "epcoh: 000150\t D_loss: 0.0039\t G_loss: 4.3998\t D_x: 0.1234\t D_g_z1: 0.0018\t D_g_z2: 0.0139\t\n",
            "epcoh: 000151\t D_loss: 0.0034\t G_loss: 4.4656\t D_x: 0.1235\t D_g_z1: 0.0016\t D_g_z2: 0.0124\t\n",
            "epcoh: 000152\t D_loss: 0.0037\t G_loss: 4.4566\t D_x: 0.1234\t D_g_z1: 0.0016\t D_g_z2: 0.0127\t\n",
            "epcoh: 000153\t D_loss: 0.0050\t G_loss: 4.2985\t D_x: 0.1228\t D_g_z1: 0.0020\t D_g_z2: 0.0154\t\n",
            "epcoh: 000154\t D_loss: 0.0049\t G_loss: 4.2509\t D_x: 0.1229\t D_g_z1: 0.0020\t D_g_z2: 0.0159\t\n",
            "epcoh: 000155\t D_loss: 0.0041\t G_loss: 4.3294\t D_x: 0.1235\t D_g_z1: 0.0019\t D_g_z2: 0.0146\t\n",
            "epcoh: 000156\t D_loss: 0.0044\t G_loss: 4.3389\t D_x: 0.1231\t D_g_z1: 0.0018\t D_g_z2: 0.0142\t\n",
            "epcoh: 000157\t D_loss: 0.0036\t G_loss: 4.5480\t D_x: 0.1234\t D_g_z1: 0.0014\t D_g_z2: 0.0114\t\n",
            "epcoh: 000158\t D_loss: 0.0031\t G_loss: 4.6875\t D_x: 0.1237\t D_g_z1: 0.0013\t D_g_z2: 0.0103\t\n",
            "epcoh: 000159\t D_loss: 0.0041\t G_loss: 4.5025\t D_x: 0.1234\t D_g_z1: 0.0015\t D_g_z2: 0.0123\t\n",
            "epcoh: 000160\t D_loss: 0.0035\t G_loss: 4.5827\t D_x: 0.1235\t D_g_z1: 0.0015\t D_g_z2: 0.0119\t\n",
            "epcoh: 000161\t D_loss: 0.0041\t G_loss: 4.4350\t D_x: 0.1234\t D_g_z1: 0.0018\t D_g_z2: 0.0133\t\n",
            "epcoh: 000162\t D_loss: 0.0032\t G_loss: 4.5429\t D_x: 0.1237\t D_g_z1: 0.0014\t D_g_z2: 0.0118\t\n",
            "epcoh: 000163\t D_loss: 0.0031\t G_loss: 4.7201\t D_x: 0.1237\t D_g_z1: 0.0012\t D_g_z2: 0.0096\t\n",
            "epcoh: 000164\t D_loss: 0.0026\t G_loss: 4.6278\t D_x: 0.1239\t D_g_z1: 0.0013\t D_g_z2: 0.0108\t\n",
            "epcoh: 000165\t D_loss: 0.0036\t G_loss: 4.4328\t D_x: 0.1235\t D_g_z1: 0.0017\t D_g_z2: 0.0131\t\n",
            "epcoh: 000166\t D_loss: 0.0037\t G_loss: 4.4850\t D_x: 0.1234\t D_g_z1: 0.0016\t D_g_z2: 0.0124\t\n",
            "epcoh: 000167\t D_loss: 0.0033\t G_loss: 4.5367\t D_x: 0.1236\t D_g_z1: 0.0015\t D_g_z2: 0.0119\t\n",
            "epcoh: 000168\t D_loss: 0.0037\t G_loss: 4.4524\t D_x: 0.1233\t D_g_z1: 0.0017\t D_g_z2: 0.0133\t\n",
            "epcoh: 000169\t D_loss: 0.0032\t G_loss: 4.7093\t D_x: 0.1235\t D_g_z1: 0.0013\t D_g_z2: 0.0101\t\n",
            "epcoh: 000170\t D_loss: 0.0041\t G_loss: 4.3981\t D_x: 0.1233\t D_g_z1: 0.0019\t D_g_z2: 0.0150\t\n",
            "epcoh: 000171\t D_loss: 0.0044\t G_loss: 4.4716\t D_x: 0.1230\t D_g_z1: 0.0017\t D_g_z2: 0.0132\t\n",
            "epcoh: 000172\t D_loss: 0.0036\t G_loss: 4.6006\t D_x: 0.1234\t D_g_z1: 0.0014\t D_g_z2: 0.0115\t\n",
            "epcoh: 000173\t D_loss: 0.0040\t G_loss: 4.4955\t D_x: 0.1234\t D_g_z1: 0.0016\t D_g_z2: 0.0125\t\n",
            "epcoh: 000174\t D_loss: 0.0039\t G_loss: 4.4819\t D_x: 0.1233\t D_g_z1: 0.0016\t D_g_z2: 0.0127\t\n",
            "epcoh: 000175\t D_loss: 0.0033\t G_loss: 4.7744\t D_x: 0.1235\t D_g_z1: 0.0012\t D_g_z2: 0.0097\t\n",
            "epcoh: 000176\t D_loss: 0.0029\t G_loss: 4.8395\t D_x: 0.1237\t D_g_z1: 0.0011\t D_g_z2: 0.0088\t\n",
            "epcoh: 000177\t D_loss: 0.0039\t G_loss: 4.5409\t D_x: 0.1233\t D_g_z1: 0.0017\t D_g_z2: 0.0125\t\n",
            "epcoh: 000178\t D_loss: 0.0039\t G_loss: 4.5290\t D_x: 0.1233\t D_g_z1: 0.0015\t D_g_z2: 0.0119\t\n",
            "epcoh: 000179\t D_loss: 0.0035\t G_loss: 4.6886\t D_x: 0.1234\t D_g_z1: 0.0013\t D_g_z2: 0.0101\t\n",
            "epcoh: 000180\t D_loss: 0.0032\t G_loss: 4.7722\t D_x: 0.1237\t D_g_z1: 0.0012\t D_g_z2: 0.0095\t\n",
            "epcoh: 000181\t D_loss: 0.0036\t G_loss: 4.6462\t D_x: 0.1236\t D_g_z1: 0.0014\t D_g_z2: 0.0109\t\n",
            "epcoh: 000182\t D_loss: 0.0033\t G_loss: 4.6449\t D_x: 0.1237\t D_g_z1: 0.0014\t D_g_z2: 0.0111\t\n",
            "epcoh: 000183\t D_loss: 0.0027\t G_loss: 4.8417\t D_x: 0.1238\t D_g_z1: 0.0011\t D_g_z2: 0.0090\t\n",
            "epcoh: 000184\t D_loss: 0.0026\t G_loss: 4.7860\t D_x: 0.1239\t D_g_z1: 0.0012\t D_g_z2: 0.0094\t\n",
            "epcoh: 000185\t D_loss: 0.0033\t G_loss: 4.6488\t D_x: 0.1236\t D_g_z1: 0.0014\t D_g_z2: 0.0108\t\n",
            "epcoh: 000186\t D_loss: 0.0040\t G_loss: 4.5530\t D_x: 0.1232\t D_g_z1: 0.0015\t D_g_z2: 0.0121\t\n",
            "epcoh: 000187\t D_loss: 0.0033\t G_loss: 4.7086\t D_x: 0.1235\t D_g_z1: 0.0013\t D_g_z2: 0.0100\t\n",
            "epcoh: 000188\t D_loss: 0.0027\t G_loss: 4.9554\t D_x: 0.1237\t D_g_z1: 0.0010\t D_g_z2: 0.0079\t\n",
            "epcoh: 000189\t D_loss: 0.0020\t G_loss: 5.0528\t D_x: 0.1241\t D_g_z1: 0.0009\t D_g_z2: 0.0074\t\n",
            "epcoh: 000190\t D_loss: 0.0033\t G_loss: 4.6374\t D_x: 0.1235\t D_g_z1: 0.0014\t D_g_z2: 0.0112\t\n",
            "epcoh: 000191\t D_loss: 0.0026\t G_loss: 4.8755\t D_x: 0.1238\t D_g_z1: 0.0011\t D_g_z2: 0.0082\t\n",
            "epcoh: 000192\t D_loss: 0.0026\t G_loss: 4.8314\t D_x: 0.1238\t D_g_z1: 0.0011\t D_g_z2: 0.0087\t\n",
            "epcoh: 000193\t D_loss: 0.0027\t G_loss: 4.8715\t D_x: 0.1237\t D_g_z1: 0.0011\t D_g_z2: 0.0089\t\n",
            "epcoh: 000194\t D_loss: 0.0031\t G_loss: 4.7995\t D_x: 0.1235\t D_g_z1: 0.0012\t D_g_z2: 0.0091\t\n",
            "epcoh: 000195\t D_loss: 0.0026\t G_loss: 4.9279\t D_x: 0.1237\t D_g_z1: 0.0010\t D_g_z2: 0.0081\t\n",
            "epcoh: 000196\t D_loss: 0.0027\t G_loss: 4.8484\t D_x: 0.1237\t D_g_z1: 0.0012\t D_g_z2: 0.0090\t\n",
            "epcoh: 000197\t D_loss: 0.0024\t G_loss: 5.0314\t D_x: 0.1238\t D_g_z1: 0.0010\t D_g_z2: 0.0077\t\n",
            "epcoh: 000198\t D_loss: 0.0022\t G_loss: 5.0246\t D_x: 0.1239\t D_g_z1: 0.0009\t D_g_z2: 0.0073\t\n",
            "epcoh: 000199\t D_loss: 0.0021\t G_loss: 5.0429\t D_x: 0.1240\t D_g_z1: 0.0009\t D_g_z2: 0.0073\t\n",
            "epcoh: 000200\t D_loss: 0.0026\t G_loss: 4.8408\t D_x: 0.1239\t D_g_z1: 0.0011\t D_g_z2: 0.0090\t\n",
            "epcoh: 000201\t D_loss: 0.0027\t G_loss: 5.0164\t D_x: 0.1237\t D_g_z1: 0.0010\t D_g_z2: 0.0082\t\n",
            "epcoh: 000202\t D_loss: 0.0018\t G_loss: 5.1845\t D_x: 0.1241\t D_g_z1: 0.0008\t D_g_z2: 0.0062\t\n",
            "epcoh: 000203\t D_loss: 0.0025\t G_loss: 4.9751\t D_x: 0.1239\t D_g_z1: 0.0011\t D_g_z2: 0.0088\t\n",
            "epcoh: 000204\t D_loss: 0.0031\t G_loss: 4.9173\t D_x: 0.1236\t D_g_z1: 0.0013\t D_g_z2: 0.0101\t\n",
            "epcoh: 000205\t D_loss: 0.0023\t G_loss: 5.0166\t D_x: 0.1240\t D_g_z1: 0.0009\t D_g_z2: 0.0074\t\n",
            "epcoh: 000206\t D_loss: 0.0019\t G_loss: 5.2377\t D_x: 0.1241\t D_g_z1: 0.0008\t D_g_z2: 0.0063\t\n",
            "epcoh: 000207\t D_loss: 0.0016\t G_loss: 5.4976\t D_x: 0.1243\t D_g_z1: 0.0007\t D_g_z2: 0.0059\t\n",
            "epcoh: 000208\t D_loss: 0.0022\t G_loss: 5.0043\t D_x: 0.1240\t D_g_z1: 0.0010\t D_g_z2: 0.0081\t\n",
            "epcoh: 000209\t D_loss: 0.0023\t G_loss: 4.9556\t D_x: 0.1240\t D_g_z1: 0.0010\t D_g_z2: 0.0081\t\n",
            "epcoh: 000210\t D_loss: 0.0021\t G_loss: 5.0755\t D_x: 0.1240\t D_g_z1: 0.0009\t D_g_z2: 0.0072\t\n",
            "epcoh: 000211\t D_loss: 0.0023\t G_loss: 5.1083\t D_x: 0.1240\t D_g_z1: 0.0009\t D_g_z2: 0.0072\t\n",
            "epcoh: 000212\t D_loss: 0.0020\t G_loss: 5.0896\t D_x: 0.1241\t D_g_z1: 0.0009\t D_g_z2: 0.0069\t\n",
            "epcoh: 000213\t D_loss: 0.0020\t G_loss: 5.1380\t D_x: 0.1241\t D_g_z1: 0.0008\t D_g_z2: 0.0064\t\n",
            "epcoh: 000214\t D_loss: 0.0019\t G_loss: 5.1400\t D_x: 0.1242\t D_g_z1: 0.0008\t D_g_z2: 0.0065\t\n",
            "epcoh: 000215\t D_loss: 0.0015\t G_loss: 5.3107\t D_x: 0.1244\t D_g_z1: 0.0007\t D_g_z2: 0.0054\t\n",
            "epcoh: 000216\t D_loss: 0.0014\t G_loss: 5.2521\t D_x: 0.1244\t D_g_z1: 0.0007\t D_g_z2: 0.0058\t\n",
            "epcoh: 000217\t D_loss: 0.0017\t G_loss: 5.5103\t D_x: 0.1242\t D_g_z1: 0.0006\t D_g_z2: 0.0050\t\n",
            "epcoh: 000218\t D_loss: 0.0018\t G_loss: 5.2881\t D_x: 0.1242\t D_g_z1: 0.0008\t D_g_z2: 0.0065\t\n",
            "epcoh: 000219\t D_loss: 0.0019\t G_loss: 5.0923\t D_x: 0.1242\t D_g_z1: 0.0009\t D_g_z2: 0.0067\t\n",
            "epcoh: 000220\t D_loss: 0.0017\t G_loss: 5.4219\t D_x: 0.1242\t D_g_z1: 0.0007\t D_g_z2: 0.0052\t\n",
            "epcoh: 000221\t D_loss: 0.0015\t G_loss: 5.4594\t D_x: 0.1243\t D_g_z1: 0.0006\t D_g_z2: 0.0048\t\n",
            "epcoh: 000222\t D_loss: 0.0015\t G_loss: 5.3136\t D_x: 0.1243\t D_g_z1: 0.0007\t D_g_z2: 0.0056\t\n",
            "epcoh: 000223\t D_loss: 0.0023\t G_loss: 4.9770\t D_x: 0.1241\t D_g_z1: 0.0010\t D_g_z2: 0.0079\t\n",
            "epcoh: 000224\t D_loss: 0.0025\t G_loss: 5.0273\t D_x: 0.1239\t D_g_z1: 0.0009\t D_g_z2: 0.0074\t\n",
            "epcoh: 000225\t D_loss: 0.0020\t G_loss: 5.0610\t D_x: 0.1241\t D_g_z1: 0.0009\t D_g_z2: 0.0071\t\n",
            "epcoh: 000226\t D_loss: 0.0021\t G_loss: 5.2581\t D_x: 0.1240\t D_g_z1: 0.0007\t D_g_z2: 0.0057\t\n",
            "epcoh: 000227\t D_loss: 0.0013\t G_loss: 5.4108\t D_x: 0.1245\t D_g_z1: 0.0006\t D_g_z2: 0.0049\t\n",
            "epcoh: 000228\t D_loss: 0.0020\t G_loss: 5.0642\t D_x: 0.1241\t D_g_z1: 0.0008\t D_g_z2: 0.0070\t\n",
            "epcoh: 000229\t D_loss: 0.0025\t G_loss: 5.0451\t D_x: 0.1241\t D_g_z1: 0.0009\t D_g_z2: 0.0073\t\n",
            "epcoh: 000230\t D_loss: 0.0023\t G_loss: 5.0611\t D_x: 0.1241\t D_g_z1: 0.0009\t D_g_z2: 0.0069\t\n",
            "epcoh: 000231\t D_loss: 0.0019\t G_loss: 5.1935\t D_x: 0.1242\t D_g_z1: 0.0008\t D_g_z2: 0.0061\t\n",
            "epcoh: 000232\t D_loss: 0.0029\t G_loss: 5.1031\t D_x: 0.1239\t D_g_z1: 0.0009\t D_g_z2: 0.0067\t\n",
            "epcoh: 000233\t D_loss: 0.0019\t G_loss: 5.1703\t D_x: 0.1243\t D_g_z1: 0.0008\t D_g_z2: 0.0064\t\n",
            "epcoh: 000234\t D_loss: 0.0019\t G_loss: 5.3314\t D_x: 0.1242\t D_g_z1: 0.0007\t D_g_z2: 0.0059\t\n",
            "epcoh: 000235\t D_loss: 0.0022\t G_loss: 5.0564\t D_x: 0.1242\t D_g_z1: 0.0009\t D_g_z2: 0.0071\t\n",
            "epcoh: 000236\t D_loss: 0.0023\t G_loss: 5.0704\t D_x: 0.1240\t D_g_z1: 0.0010\t D_g_z2: 0.0076\t\n",
            "epcoh: 000237\t D_loss: 0.0018\t G_loss: 5.2479\t D_x: 0.1242\t D_g_z1: 0.0008\t D_g_z2: 0.0061\t\n",
            "epcoh: 000238\t D_loss: 0.0021\t G_loss: 5.0220\t D_x: 0.1241\t D_g_z1: 0.0010\t D_g_z2: 0.0077\t\n",
            "epcoh: 000239\t D_loss: 0.0022\t G_loss: 5.1151\t D_x: 0.1240\t D_g_z1: 0.0009\t D_g_z2: 0.0069\t\n",
            "epcoh: 000240\t D_loss: 0.0024\t G_loss: 4.9408\t D_x: 0.1240\t D_g_z1: 0.0010\t D_g_z2: 0.0083\t\n",
            "epcoh: 000241\t D_loss: 0.0015\t G_loss: 5.4482\t D_x: 0.1244\t D_g_z1: 0.0007\t D_g_z2: 0.0055\t\n",
            "epcoh: 000242\t D_loss: 0.0020\t G_loss: 5.2201\t D_x: 0.1241\t D_g_z1: 0.0008\t D_g_z2: 0.0064\t\n",
            "epcoh: 000243\t D_loss: 0.0014\t G_loss: 5.5654\t D_x: 0.1243\t D_g_z1: 0.0005\t D_g_z2: 0.0041\t\n",
            "epcoh: 000244\t D_loss: 0.0010\t G_loss: 5.7061\t D_x: 0.1245\t D_g_z1: 0.0005\t D_g_z2: 0.0037\t\n",
            "epcoh: 000245\t D_loss: 0.0017\t G_loss: 5.0772\t D_x: 0.1243\t D_g_z1: 0.0009\t D_g_z2: 0.0071\t\n",
            "epcoh: 000246\t D_loss: 0.0017\t G_loss: 5.2817\t D_x: 0.1242\t D_g_z1: 0.0007\t D_g_z2: 0.0060\t\n",
            "epcoh: 000247\t D_loss: 0.0015\t G_loss: 5.3192\t D_x: 0.1243\t D_g_z1: 0.0007\t D_g_z2: 0.0059\t\n",
            "epcoh: 000248\t D_loss: 0.0017\t G_loss: 5.2041\t D_x: 0.1242\t D_g_z1: 0.0008\t D_g_z2: 0.0062\t\n",
            "epcoh: 000249\t D_loss: 0.0013\t G_loss: 5.4725\t D_x: 0.1244\t D_g_z1: 0.0006\t D_g_z2: 0.0045\t\n",
            "epcoh: 000250\t D_loss: 0.0016\t G_loss: 5.2834\t D_x: 0.1243\t D_g_z1: 0.0007\t D_g_z2: 0.0061\t\n",
            "epcoh: 000251\t D_loss: 0.0018\t G_loss: 5.3785\t D_x: 0.1242\t D_g_z1: 0.0007\t D_g_z2: 0.0056\t\n",
            "epcoh: 000252\t D_loss: 0.0014\t G_loss: 5.3571\t D_x: 0.1243\t D_g_z1: 0.0007\t D_g_z2: 0.0053\t\n",
            "epcoh: 000253\t D_loss: 0.0019\t G_loss: 5.1577\t D_x: 0.1242\t D_g_z1: 0.0009\t D_g_z2: 0.0069\t\n",
            "epcoh: 000254\t D_loss: 0.0023\t G_loss: 5.0708\t D_x: 0.1240\t D_g_z1: 0.0010\t D_g_z2: 0.0079\t\n",
            "epcoh: 000255\t D_loss: 0.0020\t G_loss: 5.0593\t D_x: 0.1241\t D_g_z1: 0.0009\t D_g_z2: 0.0073\t\n",
            "epcoh: 000256\t D_loss: 0.0018\t G_loss: 5.3054\t D_x: 0.1241\t D_g_z1: 0.0007\t D_g_z2: 0.0054\t\n",
            "epcoh: 000257\t D_loss: 0.0008\t G_loss: 5.7503\t D_x: 0.1246\t D_g_z1: 0.0004\t D_g_z2: 0.0034\t\n",
            "epcoh: 000258\t D_loss: 0.0012\t G_loss: 5.6052\t D_x: 0.1244\t D_g_z1: 0.0005\t D_g_z2: 0.0042\t\n",
            "epcoh: 000259\t D_loss: 0.0013\t G_loss: 5.4724\t D_x: 0.1244\t D_g_z1: 0.0006\t D_g_z2: 0.0047\t\n",
            "epcoh: 000260\t D_loss: 0.0013\t G_loss: 5.5352\t D_x: 0.1244\t D_g_z1: 0.0006\t D_g_z2: 0.0044\t\n",
            "epcoh: 000261\t D_loss: 0.0012\t G_loss: 5.7075\t D_x: 0.1244\t D_g_z1: 0.0005\t D_g_z2: 0.0037\t\n",
            "epcoh: 000262\t D_loss: 0.0013\t G_loss: 5.5796\t D_x: 0.1244\t D_g_z1: 0.0005\t D_g_z2: 0.0043\t\n",
            "epcoh: 000263\t D_loss: 0.0011\t G_loss: 5.4257\t D_x: 0.1246\t D_g_z1: 0.0007\t D_g_z2: 0.0053\t\n",
            "epcoh: 000264\t D_loss: 0.0011\t G_loss: 5.7706\t D_x: 0.1245\t D_g_z1: 0.0004\t D_g_z2: 0.0035\t\n",
            "epcoh: 000265\t D_loss: 0.0014\t G_loss: 5.4945\t D_x: 0.1243\t D_g_z1: 0.0006\t D_g_z2: 0.0044\t\n",
            "epcoh: 000266\t D_loss: 0.0009\t G_loss: 5.7197\t D_x: 0.1247\t D_g_z1: 0.0005\t D_g_z2: 0.0038\t\n",
            "epcoh: 000267\t D_loss: 0.0010\t G_loss: 5.5759\t D_x: 0.1246\t D_g_z1: 0.0005\t D_g_z2: 0.0043\t\n",
            "epcoh: 000268\t D_loss: 0.0013\t G_loss: 5.3006\t D_x: 0.1244\t D_g_z1: 0.0007\t D_g_z2: 0.0055\t\n",
            "epcoh: 000269\t D_loss: 0.0008\t G_loss: 5.9426\t D_x: 0.1246\t D_g_z1: 0.0004\t D_g_z2: 0.0031\t\n",
            "epcoh: 000270\t D_loss: 0.0010\t G_loss: 5.8096\t D_x: 0.1246\t D_g_z1: 0.0004\t D_g_z2: 0.0036\t\n",
            "epcoh: 000271\t D_loss: 0.0012\t G_loss: 5.4515\t D_x: 0.1246\t D_g_z1: 0.0007\t D_g_z2: 0.0053\t\n",
            "epcoh: 000272\t D_loss: 0.0014\t G_loss: 5.5485\t D_x: 0.1244\t D_g_z1: 0.0005\t D_g_z2: 0.0042\t\n",
            "epcoh: 000273\t D_loss: 0.0009\t G_loss: 5.8631\t D_x: 0.1246\t D_g_z1: 0.0004\t D_g_z2: 0.0031\t\n",
            "epcoh: 000274\t D_loss: 0.0007\t G_loss: 5.9922\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0028\t\n",
            "epcoh: 000275\t D_loss: 0.0013\t G_loss: 5.5942\t D_x: 0.1245\t D_g_z1: 0.0006\t D_g_z2: 0.0047\t\n",
            "epcoh: 000276\t D_loss: 0.0016\t G_loss: 5.3133\t D_x: 0.1243\t D_g_z1: 0.0007\t D_g_z2: 0.0057\t\n",
            "epcoh: 000277\t D_loss: 0.0008\t G_loss: 5.8541\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0033\t\n",
            "epcoh: 000278\t D_loss: 0.0009\t G_loss: 5.8509\t D_x: 0.1246\t D_g_z1: 0.0004\t D_g_z2: 0.0030\t\n",
            "epcoh: 000279\t D_loss: 0.0008\t G_loss: 5.8901\t D_x: 0.1246\t D_g_z1: 0.0004\t D_g_z2: 0.0029\t\n",
            "epcoh: 000280\t D_loss: 0.0008\t G_loss: 5.7549\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0034\t\n",
            "epcoh: 000281\t D_loss: 0.0008\t G_loss: 5.9818\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0030\t\n",
            "epcoh: 000282\t D_loss: 0.0010\t G_loss: 5.6882\t D_x: 0.1246\t D_g_z1: 0.0005\t D_g_z2: 0.0038\t\n",
            "epcoh: 000283\t D_loss: 0.0009\t G_loss: 5.7732\t D_x: 0.1246\t D_g_z1: 0.0004\t D_g_z2: 0.0035\t\n",
            "epcoh: 000284\t D_loss: 0.0013\t G_loss: 5.4497\t D_x: 0.1245\t D_g_z1: 0.0006\t D_g_z2: 0.0052\t\n",
            "epcoh: 000285\t D_loss: 0.0010\t G_loss: 5.6707\t D_x: 0.1246\t D_g_z1: 0.0006\t D_g_z2: 0.0043\t\n",
            "epcoh: 000286\t D_loss: 0.0012\t G_loss: 5.5158\t D_x: 0.1245\t D_g_z1: 0.0006\t D_g_z2: 0.0047\t\n",
            "epcoh: 000287\t D_loss: 0.0013\t G_loss: 5.6459\t D_x: 0.1244\t D_g_z1: 0.0006\t D_g_z2: 0.0049\t\n",
            "epcoh: 000288\t D_loss: 0.0010\t G_loss: 6.2875\t D_x: 0.1245\t D_g_z1: 0.0004\t D_g_z2: 0.0028\t\n",
            "epcoh: 000289\t D_loss: 0.0010\t G_loss: 6.2744\t D_x: 0.1246\t D_g_z1: 0.0006\t D_g_z2: 0.0041\t\n",
            "epcoh: 000290\t D_loss: 0.0010\t G_loss: 6.5681\t D_x: 0.1245\t D_g_z1: 0.0004\t D_g_z2: 0.0031\t\n",
            "epcoh: 000291\t D_loss: 0.0008\t G_loss: 5.9258\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0034\t\n",
            "epcoh: 000292\t D_loss: 0.0014\t G_loss: 5.6163\t D_x: 0.1244\t D_g_z1: 0.0007\t D_g_z2: 0.0058\t\n",
            "epcoh: 000293\t D_loss: 0.0012\t G_loss: 5.4697\t D_x: 0.1245\t D_g_z1: 0.0006\t D_g_z2: 0.0045\t\n",
            "epcoh: 000294\t D_loss: 0.0010\t G_loss: 5.5582\t D_x: 0.1246\t D_g_z1: 0.0006\t D_g_z2: 0.0044\t\n",
            "epcoh: 000295\t D_loss: 0.0009\t G_loss: 5.7450\t D_x: 0.1246\t D_g_z1: 0.0004\t D_g_z2: 0.0034\t\n",
            "epcoh: 000296\t D_loss: 0.0009\t G_loss: 5.8093\t D_x: 0.1245\t D_g_z1: 0.0004\t D_g_z2: 0.0032\t\n",
            "epcoh: 000297\t D_loss: 0.0008\t G_loss: 5.6953\t D_x: 0.1247\t D_g_z1: 0.0005\t D_g_z2: 0.0037\t\n",
            "epcoh: 000298\t D_loss: 0.0009\t G_loss: 5.6587\t D_x: 0.1247\t D_g_z1: 0.0005\t D_g_z2: 0.0037\t\n",
            "epcoh: 000299\t D_loss: 0.0012\t G_loss: 5.5086\t D_x: 0.1245\t D_g_z1: 0.0006\t D_g_z2: 0.0048\t\n",
            "epcoh: 000300\t D_loss: 0.0018\t G_loss: 5.1123\t D_x: 0.1243\t D_g_z1: 0.0008\t D_g_z2: 0.0067\t\n",
            "epcoh: 000301\t D_loss: 0.0013\t G_loss: 5.4774\t D_x: 0.1244\t D_g_z1: 0.0006\t D_g_z2: 0.0046\t\n",
            "epcoh: 000302\t D_loss: 0.0017\t G_loss: 5.0761\t D_x: 0.1244\t D_g_z1: 0.0010\t D_g_z2: 0.0073\t\n",
            "epcoh: 000303\t D_loss: 0.0014\t G_loss: 5.4663\t D_x: 0.1244\t D_g_z1: 0.0006\t D_g_z2: 0.0049\t\n",
            "epcoh: 000304\t D_loss: 0.0011\t G_loss: 5.6956\t D_x: 0.1245\t D_g_z1: 0.0005\t D_g_z2: 0.0037\t\n",
            "epcoh: 000305\t D_loss: 0.0012\t G_loss: 5.4510\t D_x: 0.1245\t D_g_z1: 0.0006\t D_g_z2: 0.0052\t\n",
            "epcoh: 000306\t D_loss: 0.0009\t G_loss: 5.6699\t D_x: 0.1246\t D_g_z1: 0.0005\t D_g_z2: 0.0037\t\n",
            "epcoh: 000307\t D_loss: 0.0013\t G_loss: 5.7824\t D_x: 0.1244\t D_g_z1: 0.0005\t D_g_z2: 0.0038\t\n",
            "epcoh: 000308\t D_loss: 0.0008\t G_loss: 6.0501\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0030\t\n",
            "epcoh: 000309\t D_loss: 0.0006\t G_loss: 6.2967\t D_x: 0.1247\t D_g_z1: 0.0002\t D_g_z2: 0.0019\t\n",
            "epcoh: 000310\t D_loss: 0.0006\t G_loss: 6.3706\t D_x: 0.1247\t D_g_z1: 0.0002\t D_g_z2: 0.0020\t\n",
            "epcoh: 000311\t D_loss: 0.0006\t G_loss: 6.0343\t D_x: 0.1248\t D_g_z1: 0.0004\t D_g_z2: 0.0030\t\n",
            "epcoh: 000312\t D_loss: 0.0013\t G_loss: 5.4559\t D_x: 0.1245\t D_g_z1: 0.0007\t D_g_z2: 0.0051\t\n",
            "epcoh: 000313\t D_loss: 0.0013\t G_loss: 5.4343\t D_x: 0.1244\t D_g_z1: 0.0006\t D_g_z2: 0.0051\t\n",
            "epcoh: 000314\t D_loss: 0.0008\t G_loss: 6.0293\t D_x: 0.1246\t D_g_z1: 0.0003\t D_g_z2: 0.0027\t\n",
            "epcoh: 000315\t D_loss: 0.0007\t G_loss: 6.2157\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0022\t\n",
            "epcoh: 000316\t D_loss: 0.0005\t G_loss: 6.5813\t D_x: 0.1247\t D_g_z1: 0.0002\t D_g_z2: 0.0014\t\n",
            "epcoh: 000317\t D_loss: 0.0005\t G_loss: 6.4231\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0019\t\n",
            "epcoh: 000318\t D_loss: 0.0006\t G_loss: 6.3577\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0022\t\n",
            "epcoh: 000319\t D_loss: 0.0009\t G_loss: 6.1000\t D_x: 0.1246\t D_g_z1: 0.0004\t D_g_z2: 0.0029\t\n",
            "epcoh: 000320\t D_loss: 0.0007\t G_loss: 5.9517\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0028\t\n",
            "epcoh: 000321\t D_loss: 0.0008\t G_loss: 5.8648\t D_x: 0.1246\t D_g_z1: 0.0004\t D_g_z2: 0.0030\t\n",
            "epcoh: 000322\t D_loss: 0.0008\t G_loss: 5.9334\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0028\t\n",
            "epcoh: 000323\t D_loss: 0.0008\t G_loss: 5.8973\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0030\t\n",
            "epcoh: 000324\t D_loss: 0.0006\t G_loss: 6.1752\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0022\t\n",
            "epcoh: 000325\t D_loss: 0.0008\t G_loss: 5.9970\t D_x: 0.1246\t D_g_z1: 0.0004\t D_g_z2: 0.0032\t\n",
            "epcoh: 000326\t D_loss: 0.0005\t G_loss: 6.2936\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0020\t\n",
            "epcoh: 000327\t D_loss: 0.0008\t G_loss: 5.9508\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0029\t\n",
            "epcoh: 000328\t D_loss: 0.0008\t G_loss: 5.6841\t D_x: 0.1247\t D_g_z1: 0.0005\t D_g_z2: 0.0041\t\n",
            "epcoh: 000329\t D_loss: 0.0012\t G_loss: 5.8484\t D_x: 0.1245\t D_g_z1: 0.0005\t D_g_z2: 0.0039\t\n",
            "epcoh: 000330\t D_loss: 0.0010\t G_loss: 5.5100\t D_x: 0.1247\t D_g_z1: 0.0006\t D_g_z2: 0.0048\t\n",
            "epcoh: 000331\t D_loss: 0.0011\t G_loss: 5.4781\t D_x: 0.1246\t D_g_z1: 0.0006\t D_g_z2: 0.0046\t\n",
            "epcoh: 000332\t D_loss: 0.0008\t G_loss: 6.0818\t D_x: 0.1246\t D_g_z1: 0.0003\t D_g_z2: 0.0024\t\n",
            "epcoh: 000333\t D_loss: 0.0005\t G_loss: 6.3107\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0019\t\n",
            "epcoh: 000334\t D_loss: 0.0008\t G_loss: 6.2113\t D_x: 0.1246\t D_g_z1: 0.0003\t D_g_z2: 0.0021\t\n",
            "epcoh: 000335\t D_loss: 0.0007\t G_loss: 6.2962\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0022\t\n",
            "epcoh: 000336\t D_loss: 0.0005\t G_loss: 6.3020\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0021\t\n",
            "epcoh: 000337\t D_loss: 0.0006\t G_loss: 6.1841\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0023\t\n",
            "epcoh: 000338\t D_loss: 0.0006\t G_loss: 5.8661\t D_x: 0.1248\t D_g_z1: 0.0004\t D_g_z2: 0.0031\t\n",
            "epcoh: 000339\t D_loss: 0.0007\t G_loss: 5.7651\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0034\t\n",
            "epcoh: 000340\t D_loss: 0.0006\t G_loss: 6.3660\t D_x: 0.1246\t D_g_z1: 0.0002\t D_g_z2: 0.0018\t\n",
            "epcoh: 000341\t D_loss: 0.0006\t G_loss: 6.4609\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0024\t\n",
            "epcoh: 000342\t D_loss: 0.0009\t G_loss: 5.5911\t D_x: 0.1247\t D_g_z1: 0.0005\t D_g_z2: 0.0044\t\n",
            "epcoh: 000343\t D_loss: 0.0010\t G_loss: 6.0941\t D_x: 0.1246\t D_g_z1: 0.0004\t D_g_z2: 0.0029\t\n",
            "epcoh: 000344\t D_loss: 0.0006\t G_loss: 6.1994\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0023\t\n",
            "epcoh: 000345\t D_loss: 0.0008\t G_loss: 5.8752\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0032\t\n",
            "epcoh: 000346\t D_loss: 0.0007\t G_loss: 6.1076\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0033\t\n",
            "epcoh: 000347\t D_loss: 0.0010\t G_loss: 5.7548\t D_x: 0.1246\t D_g_z1: 0.0005\t D_g_z2: 0.0040\t\n",
            "epcoh: 000348\t D_loss: 0.0011\t G_loss: 5.5064\t D_x: 0.1246\t D_g_z1: 0.0006\t D_g_z2: 0.0044\t\n",
            "epcoh: 000349\t D_loss: 0.0007\t G_loss: 5.8068\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0033\t\n",
            "epcoh: 000350\t D_loss: 0.0007\t G_loss: 6.0848\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0028\t\n",
            "epcoh: 000351\t D_loss: 0.0008\t G_loss: 5.9140\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0031\t\n",
            "epcoh: 000352\t D_loss: 0.0008\t G_loss: 5.8850\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0032\t\n",
            "epcoh: 000353\t D_loss: 0.0008\t G_loss: 6.2161\t D_x: 0.1246\t D_g_z1: 0.0003\t D_g_z2: 0.0026\t\n",
            "epcoh: 000354\t D_loss: 0.0005\t G_loss: 6.1169\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0023\t\n",
            "epcoh: 000355\t D_loss: 0.0008\t G_loss: 6.0175\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0030\t\n",
            "epcoh: 000356\t D_loss: 0.0008\t G_loss: 6.4089\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0026\t\n",
            "epcoh: 000357\t D_loss: 0.0007\t G_loss: 5.8863\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0033\t\n",
            "epcoh: 000358\t D_loss: 0.0006\t G_loss: 6.1280\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0024\t\n",
            "epcoh: 000359\t D_loss: 0.0008\t G_loss: 6.1957\t D_x: 0.1246\t D_g_z1: 0.0003\t D_g_z2: 0.0022\t\n",
            "epcoh: 000360\t D_loss: 0.0007\t G_loss: 6.1569\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0028\t\n",
            "epcoh: 000361\t D_loss: 0.0008\t G_loss: 5.8459\t D_x: 0.1246\t D_g_z1: 0.0004\t D_g_z2: 0.0031\t\n",
            "epcoh: 000362\t D_loss: 0.0008\t G_loss: 5.7934\t D_x: 0.1246\t D_g_z1: 0.0004\t D_g_z2: 0.0033\t\n",
            "epcoh: 000363\t D_loss: 0.0007\t G_loss: 6.0840\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0025\t\n",
            "epcoh: 000364\t D_loss: 0.0006\t G_loss: 6.0114\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0029\t\n",
            "epcoh: 000365\t D_loss: 0.0008\t G_loss: 6.0448\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0031\t\n",
            "epcoh: 000366\t D_loss: 0.0008\t G_loss: 5.9642\t D_x: 0.1246\t D_g_z1: 0.0003\t D_g_z2: 0.0027\t\n",
            "epcoh: 000367\t D_loss: 0.0007\t G_loss: 5.7231\t D_x: 0.1248\t D_g_z1: 0.0005\t D_g_z2: 0.0037\t\n",
            "epcoh: 000368\t D_loss: 0.0007\t G_loss: 6.1774\t D_x: 0.1246\t D_g_z1: 0.0003\t D_g_z2: 0.0022\t\n",
            "epcoh: 000369\t D_loss: 0.0008\t G_loss: 5.9510\t D_x: 0.1246\t D_g_z1: 0.0003\t D_g_z2: 0.0027\t\n",
            "epcoh: 000370\t D_loss: 0.0005\t G_loss: 5.9784\t D_x: 0.1248\t D_g_z1: 0.0004\t D_g_z2: 0.0028\t\n",
            "epcoh: 000371\t D_loss: 0.0008\t G_loss: 6.0055\t D_x: 0.1246\t D_g_z1: 0.0003\t D_g_z2: 0.0026\t\n",
            "epcoh: 000372\t D_loss: 0.0006\t G_loss: 6.1071\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0024\t\n",
            "epcoh: 000373\t D_loss: 0.0010\t G_loss: 5.5475\t D_x: 0.1246\t D_g_z1: 0.0006\t D_g_z2: 0.0044\t\n",
            "epcoh: 000374\t D_loss: 0.0010\t G_loss: 5.7756\t D_x: 0.1246\t D_g_z1: 0.0005\t D_g_z2: 0.0043\t\n",
            "epcoh: 000375\t D_loss: 0.0009\t G_loss: 5.8052\t D_x: 0.1246\t D_g_z1: 0.0005\t D_g_z2: 0.0041\t\n",
            "epcoh: 000376\t D_loss: 0.0012\t G_loss: 5.9017\t D_x: 0.1244\t D_g_z1: 0.0005\t D_g_z2: 0.0039\t\n",
            "epcoh: 000377\t D_loss: 0.0008\t G_loss: 6.1020\t D_x: 0.1246\t D_g_z1: 0.0003\t D_g_z2: 0.0025\t\n",
            "epcoh: 000378\t D_loss: 0.0007\t G_loss: 6.1347\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0024\t\n",
            "epcoh: 000379\t D_loss: 0.0006\t G_loss: 6.1111\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0026\t\n",
            "epcoh: 000380\t D_loss: 0.0011\t G_loss: 5.8496\t D_x: 0.1246\t D_g_z1: 0.0005\t D_g_z2: 0.0039\t\n",
            "epcoh: 000381\t D_loss: 0.0008\t G_loss: 5.8238\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0033\t\n",
            "epcoh: 000382\t D_loss: 0.0008\t G_loss: 5.9962\t D_x: 0.1246\t D_g_z1: 0.0004\t D_g_z2: 0.0030\t\n",
            "epcoh: 000383\t D_loss: 0.0006\t G_loss: 6.2564\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0024\t\n",
            "epcoh: 000384\t D_loss: 0.0006\t G_loss: 6.0240\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0026\t\n",
            "epcoh: 000385\t D_loss: 0.0008\t G_loss: 6.1338\t D_x: 0.1246\t D_g_z1: 0.0003\t D_g_z2: 0.0024\t\n",
            "epcoh: 000386\t D_loss: 0.0010\t G_loss: 5.7808\t D_x: 0.1246\t D_g_z1: 0.0005\t D_g_z2: 0.0037\t\n",
            "epcoh: 000387\t D_loss: 0.0009\t G_loss: 5.5744\t D_x: 0.1247\t D_g_z1: 0.0006\t D_g_z2: 0.0043\t\n",
            "epcoh: 000388\t D_loss: 0.0009\t G_loss: 5.8654\t D_x: 0.1246\t D_g_z1: 0.0004\t D_g_z2: 0.0031\t\n",
            "epcoh: 000389\t D_loss: 0.0004\t G_loss: 6.3147\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0020\t\n",
            "epcoh: 000390\t D_loss: 0.0005\t G_loss: 6.4004\t D_x: 0.1247\t D_g_z1: 0.0002\t D_g_z2: 0.0017\t\n",
            "epcoh: 000391\t D_loss: 0.0006\t G_loss: 5.9584\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0029\t\n",
            "epcoh: 000392\t D_loss: 0.0014\t G_loss: 5.3007\t D_x: 0.1245\t D_g_z1: 0.0008\t D_g_z2: 0.0059\t\n",
            "epcoh: 000393\t D_loss: 0.0010\t G_loss: 5.7416\t D_x: 0.1246\t D_g_z1: 0.0005\t D_g_z2: 0.0038\t\n",
            "epcoh: 000394\t D_loss: 0.0006\t G_loss: 6.0754\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0024\t\n",
            "epcoh: 000395\t D_loss: 0.0012\t G_loss: 5.4819\t D_x: 0.1245\t D_g_z1: 0.0006\t D_g_z2: 0.0047\t\n",
            "epcoh: 000396\t D_loss: 0.0007\t G_loss: 5.9600\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0028\t\n",
            "epcoh: 000397\t D_loss: 0.0007\t G_loss: 6.0630\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0027\t\n",
            "epcoh: 000398\t D_loss: 0.0009\t G_loss: 5.8856\t D_x: 0.1246\t D_g_z1: 0.0004\t D_g_z2: 0.0030\t\n",
            "epcoh: 000399\t D_loss: 0.0008\t G_loss: 5.8632\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0031\t\n",
            "epcoh: 000400\t D_loss: 0.0009\t G_loss: 5.7691\t D_x: 0.1246\t D_g_z1: 0.0005\t D_g_z2: 0.0036\t\n",
            "epcoh: 000401\t D_loss: 0.0010\t G_loss: 6.0096\t D_x: 0.1246\t D_g_z1: 0.0004\t D_g_z2: 0.0035\t\n",
            "epcoh: 000402\t D_loss: 0.0008\t G_loss: 5.9184\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0030\t\n",
            "epcoh: 000403\t D_loss: 0.0008\t G_loss: 6.0758\t D_x: 0.1246\t D_g_z1: 0.0003\t D_g_z2: 0.0024\t\n",
            "epcoh: 000404\t D_loss: 0.0006\t G_loss: 6.0891\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0025\t\n",
            "epcoh: 000405\t D_loss: 0.0007\t G_loss: 6.0903\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0027\t\n",
            "epcoh: 000406\t D_loss: 0.0005\t G_loss: 6.3313\t D_x: 0.1247\t D_g_z1: 0.0002\t D_g_z2: 0.0019\t\n",
            "epcoh: 000407\t D_loss: 0.0005\t G_loss: 6.4868\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0018\t\n",
            "epcoh: 000408\t D_loss: 0.0006\t G_loss: 6.4076\t D_x: 0.1247\t D_g_z1: 0.0002\t D_g_z2: 0.0020\t\n",
            "epcoh: 000409\t D_loss: 0.0005\t G_loss: 6.3241\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0020\t\n",
            "epcoh: 000410\t D_loss: 0.0005\t G_loss: 6.3185\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0019\t\n",
            "epcoh: 000411\t D_loss: 0.0009\t G_loss: 5.7055\t D_x: 0.1247\t D_g_z1: 0.0005\t D_g_z2: 0.0039\t\n",
            "epcoh: 000412\t D_loss: 0.0008\t G_loss: 5.9593\t D_x: 0.1246\t D_g_z1: 0.0004\t D_g_z2: 0.0031\t\n",
            "epcoh: 000413\t D_loss: 0.0007\t G_loss: 6.2162\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0022\t\n",
            "epcoh: 000414\t D_loss: 0.0006\t G_loss: 6.2320\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0021\t\n",
            "epcoh: 000415\t D_loss: 0.0005\t G_loss: 6.2270\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0022\t\n",
            "epcoh: 000416\t D_loss: 0.0004\t G_loss: 6.6825\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0014\t\n",
            "epcoh: 000417\t D_loss: 0.0005\t G_loss: 6.5357\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0016\t\n",
            "epcoh: 000418\t D_loss: 0.0004\t G_loss: 6.5014\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0016\t\n",
            "epcoh: 000419\t D_loss: 0.0005\t G_loss: 6.4833\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0019\t\n",
            "epcoh: 000420\t D_loss: 0.0005\t G_loss: 6.5094\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0020\t\n",
            "epcoh: 000421\t D_loss: 0.0005\t G_loss: 6.6186\t D_x: 0.1247\t D_g_z1: 0.0002\t D_g_z2: 0.0016\t\n",
            "epcoh: 000422\t D_loss: 0.0004\t G_loss: 6.5271\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0019\t\n",
            "epcoh: 000423\t D_loss: 0.0005\t G_loss: 6.6318\t D_x: 0.1247\t D_g_z1: 0.0002\t D_g_z2: 0.0017\t\n",
            "epcoh: 000424\t D_loss: 0.0005\t G_loss: 6.1872\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0024\t\n",
            "epcoh: 000425\t D_loss: 0.0007\t G_loss: 5.9669\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0027\t\n",
            "epcoh: 000426\t D_loss: 0.0006\t G_loss: 6.1035\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0024\t\n",
            "epcoh: 000427\t D_loss: 0.0005\t G_loss: 6.5205\t D_x: 0.1247\t D_g_z1: 0.0002\t D_g_z2: 0.0016\t\n",
            "epcoh: 000428\t D_loss: 0.0005\t G_loss: 6.5649\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0015\t\n",
            "epcoh: 000429\t D_loss: 0.0006\t G_loss: 6.1789\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0023\t\n",
            "epcoh: 000430\t D_loss: 0.0005\t G_loss: 6.3431\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0021\t\n",
            "epcoh: 000431\t D_loss: 0.0005\t G_loss: 6.2150\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0021\t\n",
            "epcoh: 000432\t D_loss: 0.0004\t G_loss: 6.5008\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0017\t\n",
            "epcoh: 000433\t D_loss: 0.0003\t G_loss: 6.6306\t D_x: 0.1249\t D_g_z1: 0.0002\t D_g_z2: 0.0017\t\n",
            "epcoh: 000434\t D_loss: 0.0005\t G_loss: 6.6160\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0017\t\n",
            "epcoh: 000435\t D_loss: 0.0007\t G_loss: 6.4584\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0029\t\n",
            "epcoh: 000436\t D_loss: 0.0006\t G_loss: 6.2602\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0024\t\n",
            "epcoh: 000437\t D_loss: 0.0005\t G_loss: 6.4253\t D_x: 0.1247\t D_g_z1: 0.0002\t D_g_z2: 0.0018\t\n",
            "epcoh: 000438\t D_loss: 0.0004\t G_loss: 6.5233\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0015\t\n",
            "epcoh: 000439\t D_loss: 0.0006\t G_loss: 6.0722\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0027\t\n",
            "epcoh: 000440\t D_loss: 0.0005\t G_loss: 6.2358\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0023\t\n",
            "epcoh: 000441\t D_loss: 0.0008\t G_loss: 5.9512\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0029\t\n",
            "epcoh: 000442\t D_loss: 0.0005\t G_loss: 6.3478\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0019\t\n",
            "epcoh: 000443\t D_loss: 0.0005\t G_loss: 6.4387\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0020\t\n",
            "epcoh: 000444\t D_loss: 0.0007\t G_loss: 6.0536\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0029\t\n",
            "epcoh: 000445\t D_loss: 0.0005\t G_loss: 6.4684\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0018\t\n",
            "epcoh: 000446\t D_loss: 0.0003\t G_loss: 6.5043\t D_x: 0.1249\t D_g_z1: 0.0002\t D_g_z2: 0.0016\t\n",
            "epcoh: 000447\t D_loss: 0.0007\t G_loss: 5.9906\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0027\t\n",
            "epcoh: 000448\t D_loss: 0.0006\t G_loss: 6.3124\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0027\t\n",
            "epcoh: 000449\t D_loss: 0.0007\t G_loss: 6.5159\t D_x: 0.1246\t D_g_z1: 0.0003\t D_g_z2: 0.0024\t\n",
            "epcoh: 000450\t D_loss: 0.0008\t G_loss: 5.9749\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0031\t\n",
            "epcoh: 000451\t D_loss: 0.0007\t G_loss: 6.1151\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0029\t\n",
            "epcoh: 000452\t D_loss: 0.0008\t G_loss: 6.0195\t D_x: 0.1246\t D_g_z1: 0.0004\t D_g_z2: 0.0030\t\n",
            "epcoh: 000453\t D_loss: 0.0009\t G_loss: 6.0979\t D_x: 0.1246\t D_g_z1: 0.0003\t D_g_z2: 0.0026\t\n",
            "epcoh: 000454\t D_loss: 0.0007\t G_loss: 5.8153\t D_x: 0.1248\t D_g_z1: 0.0004\t D_g_z2: 0.0034\t\n",
            "epcoh: 000455\t D_loss: 0.0007\t G_loss: 6.2984\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0022\t\n",
            "epcoh: 000456\t D_loss: 0.0007\t G_loss: 6.2716\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0021\t\n",
            "epcoh: 000457\t D_loss: 0.0005\t G_loss: 6.3488\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0018\t\n",
            "epcoh: 000458\t D_loss: 0.0003\t G_loss: 6.7113\t D_x: 0.1249\t D_g_z1: 0.0002\t D_g_z2: 0.0015\t\n",
            "epcoh: 000459\t D_loss: 0.0004\t G_loss: 6.8593\t D_x: 0.1248\t D_g_z1: 0.0001\t D_g_z2: 0.0011\t\n",
            "epcoh: 000460\t D_loss: 0.0003\t G_loss: 6.7523\t D_x: 0.1249\t D_g_z1: 0.0002\t D_g_z2: 0.0013\t\n",
            "epcoh: 000461\t D_loss: 0.0006\t G_loss: 6.0144\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0027\t\n",
            "epcoh: 000462\t D_loss: 0.0006\t G_loss: 5.9522\t D_x: 0.1248\t D_g_z1: 0.0004\t D_g_z2: 0.0029\t\n",
            "epcoh: 000463\t D_loss: 0.0005\t G_loss: 6.3561\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0020\t\n",
            "epcoh: 000464\t D_loss: 0.0007\t G_loss: 5.9378\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0028\t\n",
            "epcoh: 000465\t D_loss: 0.0005\t G_loss: 6.1579\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0025\t\n",
            "epcoh: 000466\t D_loss: 0.0005\t G_loss: 6.1274\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0023\t\n",
            "epcoh: 000467\t D_loss: 0.0004\t G_loss: 6.4858\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0016\t\n",
            "epcoh: 000468\t D_loss: 0.0004\t G_loss: 6.5385\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0015\t\n",
            "epcoh: 000469\t D_loss: 0.0005\t G_loss: 6.6804\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0014\t\n",
            "epcoh: 000470\t D_loss: 0.0003\t G_loss: 6.8365\t D_x: 0.1249\t D_g_z1: 0.0002\t D_g_z2: 0.0012\t\n",
            "epcoh: 000471\t D_loss: 0.0004\t G_loss: 6.2584\t D_x: 0.1249\t D_g_z1: 0.0003\t D_g_z2: 0.0021\t\n",
            "epcoh: 000472\t D_loss: 0.0006\t G_loss: 5.9951\t D_x: 0.1248\t D_g_z1: 0.0004\t D_g_z2: 0.0030\t\n",
            "epcoh: 000473\t D_loss: 0.0007\t G_loss: 6.2417\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0026\t\n",
            "epcoh: 000474\t D_loss: 0.0006\t G_loss: 6.1652\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0022\t\n",
            "epcoh: 000475\t D_loss: 0.0006\t G_loss: 6.2285\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0023\t\n",
            "epcoh: 000476\t D_loss: 0.0004\t G_loss: 6.2910\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0020\t\n",
            "epcoh: 000477\t D_loss: 0.0007\t G_loss: 6.4408\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0026\t\n",
            "epcoh: 000478\t D_loss: 0.0005\t G_loss: 6.9043\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0016\t\n",
            "epcoh: 000479\t D_loss: 0.0003\t G_loss: 6.9386\t D_x: 0.1249\t D_g_z1: 0.0001\t D_g_z2: 0.0012\t\n",
            "epcoh: 000480\t D_loss: 0.0006\t G_loss: 6.2905\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0021\t\n",
            "epcoh: 000481\t D_loss: 0.0006\t G_loss: 6.0506\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0027\t\n",
            "epcoh: 000482\t D_loss: 0.0006\t G_loss: 6.1635\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0022\t\n",
            "epcoh: 000483\t D_loss: 0.0007\t G_loss: 6.0154\t D_x: 0.1246\t D_g_z1: 0.0003\t D_g_z2: 0.0026\t\n",
            "epcoh: 000484\t D_loss: 0.0004\t G_loss: 6.2833\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0020\t\n",
            "epcoh: 000485\t D_loss: 0.0010\t G_loss: 5.9357\t D_x: 0.1246\t D_g_z1: 0.0004\t D_g_z2: 0.0029\t\n",
            "epcoh: 000486\t D_loss: 0.0008\t G_loss: 6.0173\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0033\t\n",
            "epcoh: 000487\t D_loss: 0.0008\t G_loss: 6.0001\t D_x: 0.1247\t D_g_z1: 0.0004\t D_g_z2: 0.0032\t\n",
            "epcoh: 000488\t D_loss: 0.0009\t G_loss: 6.3894\t D_x: 0.1247\t D_g_z1: 0.0002\t D_g_z2: 0.0018\t\n",
            "epcoh: 000489\t D_loss: 0.0006\t G_loss: 6.3190\t D_x: 0.1248\t D_g_z1: 0.0004\t D_g_z2: 0.0031\t\n",
            "epcoh: 000490\t D_loss: 0.0008\t G_loss: 5.9064\t D_x: 0.1246\t D_g_z1: 0.0004\t D_g_z2: 0.0029\t\n",
            "epcoh: 000491\t D_loss: 0.0009\t G_loss: 6.0976\t D_x: 0.1246\t D_g_z1: 0.0003\t D_g_z2: 0.0024\t\n",
            "epcoh: 000492\t D_loss: 0.0003\t G_loss: 6.4811\t D_x: 0.1249\t D_g_z1: 0.0002\t D_g_z2: 0.0017\t\n",
            "epcoh: 000493\t D_loss: 0.0004\t G_loss: 6.5252\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0017\t\n",
            "epcoh: 000494\t D_loss: 0.0009\t G_loss: 6.0202\t D_x: 0.1246\t D_g_z1: 0.0003\t D_g_z2: 0.0028\t\n",
            "epcoh: 000495\t D_loss: 0.0008\t G_loss: 6.3060\t D_x: 0.1247\t D_g_z1: 0.0002\t D_g_z2: 0.0020\t\n",
            "epcoh: 000496\t D_loss: 0.0005\t G_loss: 6.3759\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0020\t\n",
            "epcoh: 000497\t D_loss: 0.0006\t G_loss: 6.1519\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0024\t\n",
            "epcoh: 000498\t D_loss: 0.0007\t G_loss: 6.1140\t D_x: 0.1247\t D_g_z1: 0.0003\t D_g_z2: 0.0025\t\n",
            "epcoh: 000499\t D_loss: 0.0008\t G_loss: 6.4380\t D_x: 0.1246\t D_g_z1: 0.0003\t D_g_z2: 0.0020\t\n",
            "epcoh: 000500\t D_loss: 0.0004\t G_loss: 6.8693\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0012\t\n",
            "epcoh: 000501\t D_loss: 0.0007\t G_loss: 6.7317\t D_x: 0.1247\t D_g_z1: 0.0002\t D_g_z2: 0.0018\t\n",
            "epcoh: 000502\t D_loss: 0.0004\t G_loss: 6.7311\t D_x: 0.1248\t D_g_z1: 0.0003\t D_g_z2: 0.0020\t\n",
            "epcoh: 000503\t D_loss: 0.0004\t G_loss: 6.7919\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0013\t\n",
            "epcoh: 000504\t D_loss: 0.0003\t G_loss: 6.6993\t D_x: 0.1249\t D_g_z1: 0.0002\t D_g_z2: 0.0014\t\n",
            "epcoh: 000505\t D_loss: 0.0003\t G_loss: 6.8394\t D_x: 0.1249\t D_g_z1: 0.0001\t D_g_z2: 0.0011\t\n",
            "epcoh: 000506\t D_loss: 0.0004\t G_loss: 6.7509\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0012\t\n",
            "epcoh: 000507\t D_loss: 0.0004\t G_loss: 6.8687\t D_x: 0.1248\t D_g_z1: 0.0001\t D_g_z2: 0.0011\t\n",
            "epcoh: 000508\t D_loss: 0.0004\t G_loss: 6.8383\t D_x: 0.1248\t D_g_z1: 0.0001\t D_g_z2: 0.0012\t\n",
            "epcoh: 000509\t D_loss: 0.0003\t G_loss: 6.7663\t D_x: 0.1249\t D_g_z1: 0.0002\t D_g_z2: 0.0013\t\n",
            "epcoh: 000510\t D_loss: 0.0002\t G_loss: 6.9445\t D_x: 0.1249\t D_g_z1: 0.0001\t D_g_z2: 0.0011\t\n",
            "epcoh: 000511\t D_loss: 0.0005\t G_loss: 6.4790\t D_x: 0.1248\t D_g_z1: 0.0002\t D_g_z2: 0.0017\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated = generator(test_set).detach().cpu().view(-1, 1, 28, 28)\n",
        "\n",
        "grid = torchvision.utils.make_grid(\n",
        "    generated,\n",
        "    nrow=4,\n",
        "    padding=10,\n",
        "    pad_value=1\n",
        ")\n",
        "\n",
        "img = np.transpose(\n",
        "    grid.numpy(),\n",
        "    (1, 2, 0)\n",
        ")\n",
        "\n",
        "fig = plt.figure(figsize=(16, 16))\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(img);"
      ],
      "metadata": {
        "id": "3mm92iRskbp_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "450e9a10-02ff-4606-c8b6-edb529403648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x1152 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_inception_score(images, batch_size):\n",
        "\n",
        "    # initialize pretrained inception network\n",
        "    net = inception_v3(pretrained=True).to(device)\n",
        "\n",
        "    # list of scores\n",
        "    scores = []\n",
        "\n",
        "    # number of steps\n",
        "    num_steps = int(math.ceil(float(len(images)) / float(batch_size)))\n",
        "\n",
        "    # iterate over the images\n",
        "    for i in range(num_steps):\n",
        "\n",
        "        # mini-batch start and end index\n",
        "        s = i * batch_size\n",
        "        e = (i + 1) * batch_size\n",
        "\n",
        "        # mini-batch images\n",
        "        mini_batch = images[s:e]\n",
        "        \n",
        "        # mini-batch as Torch tensor with gradients\n",
        "        batch = Variable(mini_batch)\n",
        "\n",
        "        # apply a forward pass through inception network\n",
        "        # skipping aux logits\n",
        "        '''\n",
        "        This network is unique because it has two output layers when training.\n",
        "        The second output is known as an auxiliary output and is contained in the AuxLogits part of the network.\n",
        "        The primary output is a linear layer at the end of the network.\n",
        "        Note, when testing we only consider the primary output.\n",
        "        '''\n",
        "        s, _ = net(batch)\n",
        "\n",
        "        # accumulate scores\n",
        "        scores.append(s)\n",
        "    \n",
        "    # stack scores as tensor\n",
        "    scores = torch.cat(scores, 0)\n",
        "\n",
        "    # calculate inception score\n",
        "\n",
        "    '''\n",
        "    The formula for inception score\n",
        "    IS(x) = E[ KL( P(y|x) || P(y)) ]\n",
        "    x: generated images\n",
        "    y: inception model classification distribution aka softmax\n",
        "    '''\n",
        "\n",
        "    # calculate p(y|x) across dimension 1\n",
        "    # that is one row for each image\n",
        "    p_yx = F.softmax(scores, 1)\n",
        "\n",
        "    # calculate p(y) across dimension 0\n",
        "    # that is one column for each class / label\n",
        "    p_y = p_yx.mean(0).unsqueeze(0).expand(p_yx.size(0), -1)\n",
        "\n",
        "    # calculate KL divergence\n",
        "    KL_d = p_yx * (torch.log(p_yx) - torch.log(p_y))\n",
        "\n",
        "    # calculate mean aka expected of KL\n",
        "    final_score = KL_d.mean()\n",
        "\n",
        "    # return final score\n",
        "    return final_score"
      ],
      "metadata": {
        "id": "RX74NeLwDK49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inception v3 feature extractor\n",
        "class PartialInceptionNetwork(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "      # trigger parent constructor\n",
        "      super(PartialInceptionNetwork, self).__init__()\n",
        "\n",
        "      # initialize pretrained network\n",
        "      self.inception_network = inception_v3(pretrained=True)\n",
        "      \n",
        "      # register a forward hook otherwise not implemented exception is raised\n",
        "      self.inception_network.Mixed_7c.register_forward_hook(self.output_hook)\n",
        "\n",
        "  # register hook to allow forward pass on second to last block\n",
        "  def output_hook(self, module, input, output):\n",
        "      # N x 2048 x 8 x 8\n",
        "      self.mixed_7c_output = output\n",
        "\n",
        "  def forward(self, x):\n",
        "      \"\"\"\n",
        "      Args:\n",
        "          x: shape (N, 3, 299, 299) dtype: torch.float32 in range 0-1\n",
        "      Returns:\n",
        "          inception activations: torch.tensor, shape: (N, 2048), dtype: torch.float32\n",
        "      \"\"\"\n",
        "      assert x.shape[1:] == (3, 299, 299), \"Expected input shape to be: (N,3,299,299)\" +\\\n",
        "                                            \", but got {}\".format(x.shape)\n",
        "\n",
        "      # Trigger output hook\n",
        "      self.inception_network(x)\n",
        "\n",
        "      # Output: N x 2048 x 8 x 8 \n",
        "      activations = self.mixed_7c_output\n",
        "\n",
        "      # Output: N x 2048 x 1 x 1 \n",
        "      activations = torch.nn.functional.adaptive_avg_pool2d(\n",
        "          activations, (1,1))\n",
        "\n",
        "      # Rectify dimensions\n",
        "      activations = activations.view(x.shape[0], 2048)\n",
        "\n",
        "      return activations\n",
        "\n",
        "def calc_fid_score(real_images, gen_images, batch_size):\n",
        "\n",
        "    # initialize pretrained inception network\n",
        "    net = PartialInceptionNetwork().to(device)\n",
        "\n",
        "    # list of features\n",
        "    real_activations = []\n",
        "    gen_activations = []\n",
        "\n",
        "    # number of steps\n",
        "    num_steps = int(math.ceil(float(len(real_images)) / float(batch_size)))\n",
        "\n",
        "    # iterate over the images\n",
        "    for i in range(num_steps):\n",
        "\n",
        "        # mini-batch start and end index\n",
        "        s = i * batch_size\n",
        "        e = (i + 1) * batch_size\n",
        "\n",
        "        # mini-batch images\n",
        "        mini_batch_real = real_images[s:e]\n",
        "        mini_batch_gen = gen_images[s:e]\n",
        "        \n",
        "        # mini-batch as Torch tensor with gradients\n",
        "        batch_real = Variable(mini_batch_real)\n",
        "        batch_gen = Variable(mini_batch_gen)\n",
        "\n",
        "        # apply a forward pass through inception network\n",
        "        features_real = net(batch_real)\n",
        "        features_gen = net(batch_gen)\n",
        "\n",
        "        # accumulate features\n",
        "        real_activations.append(features_real)\n",
        "        gen_activations.append(features_gen)\n",
        "    \n",
        "    # stack tensor\n",
        "    features_real = torch.cat(real_activations, 0)\n",
        "    features_gen = torch.cat(gen_activations, 0)\n",
        "\n",
        "    # tensor to numpy\n",
        "    xr = features_real.detach().cpu().numpy()\n",
        "    xg = features_gen.detach().cpu().numpy()\n",
        "\n",
        "    # calculate mean\n",
        "    u1 = np.mean(xr, axis=0)\n",
        "    u2 = np.mean(xg, axis=0)\n",
        "\n",
        "    # calculate variance\n",
        "    s1 = np.cov(xr, rowvar=False)\n",
        "    s2 = np.cov(xg, rowvar=False)\n",
        "\n",
        "    # difference squared\n",
        "    diff = u1 - u2\n",
        "    diff_squared = diff.dot(diff)\n",
        "\n",
        "    # trace covariance product\n",
        "    prod = s1.dot(s2)\n",
        "    sqrt_prod, _ = scipy.linalg.sqrtm(prod, disp=False)\n",
        "    \n",
        "    # avoid imaginary numbers\n",
        "    if np.iscomplexobj(sqrt_prod):\n",
        "        sqrt_prod = sqrt_prod.real\n",
        "\n",
        "    prod_tr = np.trace(sqrt_prod)\n",
        "\n",
        "    # calculate FID\n",
        "    final_score = diff_squared + np.trace(s1) + np.trace(s2) - 2 * prod_tr\n",
        "\n",
        "    # return final score\n",
        "    return final_score"
      ],
      "metadata": {
        "id": "iSeeW601feWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_images(imgs):\n",
        "    imgs = imgs.view(-1, 1, 28, 28)\n",
        "\n",
        "    # repeat Gray channel to RGB\n",
        "    imgs = imgs.repeat(1, 3, 1, 1)\n",
        "\n",
        "    # resize the images to 3x299x299\n",
        "    res_imgs = F.interpolate(imgs, size=(299, 299))\n",
        "\n",
        "    return res_imgs\n",
        "\n",
        "def calc_scores(imgs_real, \n",
        "                imgs_fake, \n",
        "                batch_size):\n",
        "\n",
        "    res_imgs_real = resize_images(imgs_real)\n",
        "    res_imgs_fake = resize_images(imgs_fake)\n",
        "\n",
        "    # calculate inception score\n",
        "    inception_score_real = calc_inception_score(res_imgs_real, BATCH_SIZE).item()\n",
        "    inception_score_fake = calc_inception_score(res_imgs_fake, BATCH_SIZE).item()\n",
        "\n",
        "    # calculate FID score\n",
        "    fid_score = calc_fid_score(res_imgs_real, res_imgs_fake, BATCH_SIZE)\n",
        "\n",
        "    return inception_score_real, inception_score_fake, fid_score\n",
        "\n",
        "scores = calc_scores(\n",
        "    imgs_real=next(iter(loader_train))[0].to(device), \n",
        "    imgs_fake=generator(test_set), \n",
        "    batch_size=BATCH_SIZE\n",
        "    )\n",
        "\n",
        "\n",
        "print(\"Real Inception score: {}\".format(scores[0]))\n",
        "print(\"Fake Inception score: {}\".format(scores[1]))\n",
        "print(\"FID score: {}\".format(scores[2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "ccd259848e2c42e6b609c92c534be628",
            "0a7445bd418444cba4fd1aa358e2611e",
            "ae6f0a26f6434c22895f9a8d3287262c",
            "d64fdf20a6d54b1197ee8a0e6fd47d2f",
            "78c9c45d45ab45e88ce7ee1e08a836a9",
            "b169280d112d4530a8699d0b36d89be5",
            "1b87a7f4706d4cb9a0d7dc3749eb4be8",
            "6f175bdf38374b2d90611dde88449189",
            "62a30c68f41048069e71b28d8219d50a",
            "e4f6134633ee47f5b64cdfe1575d6dd4",
            "fd7ed8f437cd464eb9db51db478eebdf"
          ]
        },
        "id": "k3o8Pfquhuiv",
        "outputId": "20deaac1-1559-4234-8744-b860285cca9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/104M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ccd259848e2c42e6b609c92c534be628"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Real Inception score: 0.0006210809224285185\n",
            "Fake Inception score: 0.00034659559605643153\n",
            "FID score: 123.66990310356746\n"
          ]
        }
      ]
    }
  ]
}