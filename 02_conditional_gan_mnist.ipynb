{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPf3MeRH5hI7EnzI5+YjIkE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lohaoxi/basic-pytorch-gans/blob/master/02_conditional_gan_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DcHvlBr1zCAk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import math\n",
        "\n",
        "import scipy.linalg\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision.models import inception_v3\n",
        "\n",
        "if not os.path.exists('visuals'):\n",
        "    os.mkdir('visuals')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "K = 4\n",
        "N_EPOCHS = 4096\n",
        "NOISE_DIM = 100\n",
        "IMAGE_DIM = 28*28\n",
        "MAXOUT_SIZE = 5\n",
        "HIDDEN_DIM = (240, 240)\n",
        "LABEL_DIM = 10"
      ],
      "metadata": {
        "id": "miLLXK9bliDw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class FlattenTransform:\n",
        "    \n",
        "    def __call__(self, inputs):\n",
        "        return inputs.view(inputs.shape[0], -1)\n",
        "\n",
        "data_train = torchvision.datasets.MNIST(\n",
        "    \"./data/mnist\", \n",
        "    train=True, \n",
        "    download=True,\n",
        "    transform=torchvision.transforms.Compose([\n",
        "        torchvision.transforms.ToTensor(), \n",
        "        FlattenTransform()\n",
        "        ])\n",
        "    )\n",
        "\n",
        "loader_train = torch.utils.data.DataLoader(\n",
        "    data_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        "    )"
      ],
      "metadata": {
        "id": "kumOTwAh6PV_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of observation: {0}\".format(len(data_train)))\n",
        "print(\"Size of each observation: {0}\".format(np.array(data_train[0][0]).shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jY6oq1uV7hlQ",
        "outputId": "cc49aa00-67ff-4d94-8743-7a10a05b88da"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of observation: 60000\n",
            "Size of each observation: (1, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class Maxout(nn.Module):\n",
        "\n",
        "    def __init__(self, n_pieces):\n",
        "        super(Maxout, self).__init__()\n",
        "        self.n_pieces = n_pieces\n",
        "        \n",
        "    def forward(self, batch):\n",
        "        assert batch.shape[1] % self.n_pieces == 0\n",
        "        batch = batch.view(\n",
        "            batch.shape[0], \n",
        "            batch.shape[1] // self.n_pieces, \n",
        "            self.n_pieces\n",
        "            )\n",
        "        batch, _ = batch.max(dim=2)\n",
        "        return batch\n",
        "    \n",
        "\n",
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, noise_dim, lbl_dim, hid_dim, out_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.noise_dim = noise_dim\n",
        "        self.lbl_dim = lbl_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.out_dim = out_dim\n",
        "\n",
        "        self.fc1 = nn.Linear(self.noise_dim + self.lbl_dim, self.hid_dim, bias=True)\n",
        "        self.fc2 = nn.Linear(self.hid_dim, self.hid_dim, bias=True)\n",
        "        self.fc3 = nn.Linear(self.hid_dim, self.hid_dim, bias=True)\n",
        "        self.fc4 = nn.Linear(self.hid_dim, self.hid_dim, bias=True)\n",
        "        self.fc5 = nn.Linear(self.hid_dim, self.hid_dim, bias=True)\n",
        "        self.fc6 = nn.Linear(self.hid_dim, self.out_dim, bias=True)\n",
        "\n",
        "    def forward(self, batch, label):\n",
        "\n",
        "        batch = batch.view(batch.size(0), -1)\n",
        "        batch = torch.cat((batch, label), dim=1)\n",
        "\n",
        "        batch = F.dropout(F.relu(self.fc1(batch)))\n",
        "        batch = F.dropout(F.relu(self.fc2(batch)))\n",
        "        batch = F.dropout(F.relu(self.fc3(batch)))\n",
        "        batch = F.dropout(F.relu(self.fc4(batch)))\n",
        "        batch = F.dropout(F.relu(self.fc5(batch)))\n",
        "\n",
        "        batch = torch.sigmoid(self.fc6(batch))\n",
        "\n",
        "        return batch\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, lbl_dim, hid_dim, out_dim, maxout_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.lbl_dim = lbl_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.maxout_size = maxout_size\n",
        "        self.Maxout = Maxout(maxout_size)\n",
        "\n",
        "        self.fc1 = nn.Linear(self.in_dim + self.lbl_dim, self.hid_dim, bias=True)\n",
        "        self.fc2 = nn.Linear(self.hid_dim // self.maxout_size, self.hid_dim, bias=True)\n",
        "        self.fc3 = nn.Linear(self.hid_dim // self.maxout_size, self.hid_dim, bias=True)\n",
        "        self.fc4 = nn.Linear(self.hid_dim // self.maxout_size, self.out_dim, bias=True)\n",
        "\n",
        "    def forward(self, batch, label):\n",
        "\n",
        "        batch = batch.view(batch.size(0), -1)\n",
        "        batch = torch.cat((batch, label), dim=1)\n",
        "\n",
        "        batch = self.Maxout(self.fc1(batch))\n",
        "        batch = self.Maxout(self.fc2(batch))\n",
        "        batch = self.Maxout(self.fc3(batch))\n",
        "        batch = torch.sigmoid(self.fc4(batch))\n",
        "        \n",
        "        return batch"
      ],
      "metadata": {
        "id": "6MVk_7s_7sxE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Generator(NOISE_DIM, LABEL_DIM, HIDDEN_DIM[0], IMAGE_DIM).to(device)\n",
        "discriminator = Discriminator(IMAGE_DIM, LABEL_DIM, HIDDEN_DIM[1], 1, MAXOUT_SIZE).to(device)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"Number of parameters in the Generatoris: {}\".format(count_parameters(generator)))\n",
        "print(\"Number of parameters in the Discriminator: {}\".format(count_parameters(discriminator)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9idKsjMG7vb_",
        "outputId": "202f94f4-1623-47a2-e437-10f9cd178e4c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters in the Generatoris: 446944\n",
            "Number of parameters in the Discriminator: 214369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encodeOneHot(lbls, lbl_dim):\n",
        "    ret = torch.FloatTensor(lbls.shape[0], lbl_dim)\n",
        "    ret.zero_()\n",
        "    ret.scatter_(dim=1, index=lbls.view(-1, 1), value=1)\n",
        "    return ret"
      ],
      "metadata": {
        "id": "hSdEtBMm5akc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lbls_real = torch.ones(BATCH_SIZE, 1).to(device)\n",
        "lbls_fake = torch.zeros(BATCH_SIZE, 1).to(device)\n",
        "\n",
        "test_z = (2 * torch.randn(10, NOISE_DIM) - 1).to(device)\n",
        "test_y = encodeOneHot(lbls=torch.tensor(np.arange(0, 10)), \n",
        "                      lbl_dim=LABEL_DIM).to(device)\n",
        "\n",
        "num_steps = len(loader_train) // BATCH_SIZE\n",
        "\n",
        "discriminator_optimizer = torch.optim.SGD(\n",
        "    discriminator.parameters(),\n",
        "    lr=0.002,\n",
        "    momentum=0.7\n",
        ")\n",
        "\n",
        "generator_optimizer = torch.optim.SGD(\n",
        "    generator.parameters(),\n",
        "    lr=0.002,\n",
        "    momentum=0.7\n",
        ")\n",
        "\n",
        "criterion = torch.nn.BCELoss()"
      ],
      "metadata": {
        "id": "WKl7ZKSW-ylP"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualizeGAN(imgs, lbls, epoch):\n",
        "\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(20, 18))\n",
        "    \n",
        "    fig.suptitle('Epoch {}'.format(str(epoch).zfill(4)))\n",
        "\n",
        "    for row, axe in enumerate(axes):\n",
        "        for col, cell in enumerate(axe):\n",
        "            cell.imshow(\n",
        "                imgs[row * 5 + col],\n",
        "                cmap='gray'\n",
        "            )\n",
        "            \n",
        "            cell.set_title('{}'.format(\n",
        "                torch.argmax(lbls[row * 5 + col])\n",
        "            ))\n",
        "\n",
        "            cell.axis(\"off\")\n",
        "\n",
        "\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    fig.savefig(os.path.join(\"visuals\", \"{}.jpg\".format(str(epoch).zfill(6))))\n",
        "    \n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "9pnTy7Qt9jV1"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_loss_ls = []\n",
        "g_loss_ls = []\n",
        "d_lr_ls = []\n",
        "g_lr_ls = []\n",
        "\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    # Loss Log\n",
        "    d_counter = 0\n",
        "    g_counter = 0\n",
        "    d_loss = 0\n",
        "    g_loss = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(loader_train):\n",
        "\n",
        "        if i == num_steps:\n",
        "            break\n",
        "\n",
        "        # Train Discriminator\n",
        "        for _ in range(4):\n",
        "        \n",
        "            real_images = images.to(device)\n",
        "            real_conditions = encodeOneHot(labels, LABEL_DIM).to(device)\n",
        "            \n",
        "            fake_conditions = encodeOneHot(torch.randint(0, 10, (BATCH_SIZE,)), LABEL_DIM).to(device)\n",
        "\n",
        "            fake_images = generator(\n",
        "                (2 * torch.randn(BATCH_SIZE, NOISE_DIM) - 1).to(device),\n",
        "                fake_conditions\n",
        "            )\n",
        "\n",
        "            discriminator_optimizer.zero_grad()\n",
        "            \n",
        "            real_outputs = discriminator(real_images, real_conditions)\n",
        "            fake_outputs = discriminator(fake_images, fake_conditions)\n",
        "            \n",
        "            d_x = criterion(real_outputs, lbls_real)\n",
        "            d_g_z = criterion(fake_outputs, lbls_fake)\n",
        "\n",
        "            d_x.backward()\n",
        "            d_g_z.backward()\n",
        "\n",
        "            discriminator_optimizer.step()\n",
        "            \n",
        "            # Loss Log\n",
        "            d_counter += 1\n",
        "            d_loss = d_x.item() + d_g_z.item()\n",
        "\n",
        "\n",
        "        # Train Generator\n",
        "        z = (2 * torch.randn(BATCH_SIZE, NOISE_DIM) - 1).to(device)\n",
        "        y = encodeOneHot(torch.randint(0, 10, (BATCH_SIZE,)), LABEL_DIM).to(device)\n",
        "\n",
        "        generator.zero_grad()\n",
        "\n",
        "        outputs = discriminator(generator(z, y), y)\n",
        "\n",
        "        loss = criterion(outputs, lbls_real)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        generator_optimizer.step()\n",
        "        \n",
        "        # LR Decay\n",
        "#         discriminator_scheduler.step()\n",
        "#         generator_scheduler.step()\n",
        "        \n",
        "        # Loss Log\n",
        "        g_counter += 1\n",
        "        g_loss += loss.item()\n",
        "\n",
        "    # Loss Log\n",
        "    if epoch % 10 == 0:\n",
        "        print(\n",
        "            'e:{}, G:{:.3f}, D:{:.3f}'.format(\n",
        "                epoch,\n",
        "                g_loss / g_counter,\n",
        "                d_loss / d_counter\n",
        "#                 generator_scheduler.get_lr(),\n",
        "#                 discriminator_scheduler.get_lr()\n",
        "            )\n",
        "        )\n",
        "    \n",
        "    # Loss Log for Plot\n",
        "    g_loss_ls.append(g_loss / g_counter)\n",
        "    d_loss_ls.append(d_loss / d_counter)\n",
        "    \n",
        "    # Learning Rate Decay Log\n",
        "#     g_lr_ls.append(generator_scheduler.get_lr())\n",
        "#     d_lr_ls.append(discriminator_scheduler.get_lr())\n",
        "\n",
        "\n",
        "    # Visualize Results\n",
        "    if epoch % 5 == 0:\n",
        "\n",
        "        generated = generator(test_z, test_y).detach().cpu().view(-1, 28, 28)\n",
        "\n",
        "        visualizeGAN(generated, test_y, epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_xAX7dyE6JO",
        "outputId": "aa06653c-c0d8-4fce-c6c9-cde575ab2f63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e:0, G:1.540, D:0.017\n",
            "e:10, G:6.016, D:0.000\n",
            "e:20, G:2.799, D:0.005\n",
            "e:30, G:5.766, D:0.000\n",
            "e:40, G:6.221, D:0.000\n",
            "e:50, G:6.856, D:0.001\n",
            "e:60, G:6.781, D:0.000\n",
            "e:70, G:7.638, D:0.000\n",
            "e:80, G:7.727, D:0.000\n",
            "e:90, G:4.411, D:0.001\n",
            "e:100, G:7.090, D:0.000\n",
            "e:110, G:9.897, D:0.000\n",
            "e:120, G:8.455, D:0.000\n",
            "e:130, G:9.367, D:0.000\n",
            "e:140, G:8.567, D:0.000\n",
            "e:150, G:8.290, D:0.000\n",
            "e:160, G:7.698, D:0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    D_loss = 0\n",
        "    G_loss = 0\n",
        "    D_x = 0\n",
        "    D_g_z1 = 0\n",
        "    D_g_z2 = 0\n",
        "\n",
        "    for i, (imgs, lbls) in enumerate(loader_train):\n",
        "\n",
        "        D_counter = 0\n",
        "        G_counter = 0\n",
        "\n",
        "        if i == num_steps: break\n",
        "\n",
        "        # Train Discriminator\n",
        "\n",
        "        for _ in range(K):\n",
        "            \n",
        "            conds_real = encodeOneHot(lbls=lbls, \n",
        "                                      lbl_dim=LABEL_DIM).to(device)\n",
        "\n",
        "            imgs_real = imgs.to(device)\n",
        "\n",
        "            conds_fake = encodeOneHot(lbls=torch.randint(0, 10, (BATCH_SIZE,)), \n",
        "                                      lbl_dim=LABEL_DIM).to(device)\n",
        "\n",
        "            imgs_fake = generator(batch=(2 * torch.randn(BATCH_SIZE, NOISE_DIM) - 1).to(device), \n",
        "                                  label=conds_fake)\n",
        "            \n",
        "\n",
        "            discriminator_optimizer.zero_grad()\n",
        "\n",
        "            outs_real = discriminator(imgs_real, conds_real)\n",
        "            outs_fake = discriminator(imgs_fake, conds_fake)\n",
        "\n",
        "            discriminator_err_real = criterion(outs_real, lbls_real)\n",
        "            discriminator_err_fake = criterion(outs_fake, lbls_fake)\n",
        "\n",
        "            discriminator_err_real.backward()\n",
        "            discriminator_err_fake.backward()\n",
        "\n",
        "            discriminator_optimizer.step()\n",
        "            \n",
        "            D_loss += (discriminator_err_real.item() + discriminator_err_fake.item())\n",
        "            D_g_z1 += outs_fake.mean().item()\n",
        "            D_g_z2 += outs.mean().item()\n",
        "            D_counter += 1\n",
        "\n",
        "        # Train Generator\n",
        "\n",
        "        z = (2 * torch.randn(BATCH_SIZE, NOISE_DIM) - 1).to(device)\n",
        "\n",
        "        y = encodeOneHot(lbls=torch.randint(0, 10, (BATCH_SIZE,)), \n",
        "                         lbl_dim=LABEL_DIM).to(device)\n",
        "\n",
        "        generator.zero_grad()\n",
        "\n",
        "        outs = discriminator(generator(z, y), y)\n",
        "\n",
        "        generator_err = criterion(outs, lbls_real)\n",
        "\n",
        "        generator_err.backward()\n",
        "\n",
        "        generator_optimizer.step()\n",
        "\n",
        "        G_loss += generator_err.item()\n",
        "        D_x += outs_real.mean().item()\n",
        "        G_counter += 1\n",
        "\n",
        "\n",
        "    print(\"epcoh: {}\\t D_loss: {:.4f}\\t G_loss: {:.4f}\\t D_x: {:.4f}\\t D_g_z1: {:.4f}\\t D_g_z2: {:.4f}\\t\".format(\n",
        "        str(epoch).zfill(6), \n",
        "        D_loss / D_counter,\n",
        "        G_loss / G_counter,\n",
        "        D_x / D_counter,\n",
        "        D_g_z1 / D_counter,\n",
        "        D_g_z2 / G_counter,\n",
        "    )\n",
        "    )\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "\n",
        "        generated = generator(test_z, test_y).detach().cpu().view(-1, 28, 28)\n",
        "\n",
        "        visualizeGAN(generated, test_y, epoch)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YLaIiWN5jFzn",
        "outputId": "68b2af0e-c590-43e1-9170-a3636fc54ced"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epcoh: 000000\t D_loss: 1.1294\t G_loss: 1.1701\t D_x: 0.1233\t D_g_z1: 0.3376\t D_g_z2: 1.2422\t\n",
            "epcoh: 000001\t D_loss: 0.6595\t G_loss: 2.2695\t D_x: 0.1478\t D_g_z1: 0.1135\t D_g_z2: 0.4672\t\n",
            "epcoh: 000002\t D_loss: 0.3138\t G_loss: 3.2330\t D_x: 0.1925\t D_g_z1: 0.0422\t D_g_z2: 0.1726\t\n",
            "epcoh: 000003\t D_loss: 0.1338\t G_loss: 4.0181\t D_x: 0.2238\t D_g_z1: 0.0190\t D_g_z2: 0.0777\t\n",
            "epcoh: 000004\t D_loss: 0.0654\t G_loss: 4.6523\t D_x: 0.2369\t D_g_z1: 0.0099\t D_g_z2: 0.0402\t\n",
            "epcoh: 000005\t D_loss: 0.0373\t G_loss: 5.1339\t D_x: 0.2425\t D_g_z1: 0.0060\t D_g_z2: 0.0245\t\n",
            "epcoh: 000006\t D_loss: 0.0256\t G_loss: 5.5083\t D_x: 0.2448\t D_g_z1: 0.0041\t D_g_z2: 0.0167\t\n",
            "epcoh: 000007\t D_loss: 0.0192\t G_loss: 5.7919\t D_x: 0.2461\t D_g_z1: 0.0031\t D_g_z2: 0.0125\t\n",
            "epcoh: 000008\t D_loss: 0.0153\t G_loss: 6.0293\t D_x: 0.2468\t D_g_z1: 0.0024\t D_g_z2: 0.0098\t\n",
            "epcoh: 000009\t D_loss: 0.0117\t G_loss: 6.2248\t D_x: 0.2476\t D_g_z1: 0.0020\t D_g_z2: 0.0081\t\n",
            "epcoh: 000010\t D_loss: 0.0096\t G_loss: 6.3990\t D_x: 0.2481\t D_g_z1: 0.0017\t D_g_z2: 0.0067\t\n",
            "epcoh: 000011\t D_loss: 0.0084\t G_loss: 6.5421\t D_x: 0.2483\t D_g_z1: 0.0015\t D_g_z2: 0.0058\t\n",
            "epcoh: 000012\t D_loss: 0.0075\t G_loss: 6.6534\t D_x: 0.2485\t D_g_z1: 0.0013\t D_g_z2: 0.0052\t\n",
            "epcoh: 000013\t D_loss: 0.0065\t G_loss: 6.7414\t D_x: 0.2487\t D_g_z1: 0.0012\t D_g_z2: 0.0048\t\n",
            "epcoh: 000014\t D_loss: 0.0060\t G_loss: 6.8017\t D_x: 0.2488\t D_g_z1: 0.0011\t D_g_z2: 0.0045\t\n",
            "epcoh: 000015\t D_loss: 0.0057\t G_loss: 6.7821\t D_x: 0.2489\t D_g_z1: 0.0012\t D_g_z2: 0.0046\t\n",
            "epcoh: 000016\t D_loss: 0.0059\t G_loss: 6.5550\t D_x: 0.2490\t D_g_z1: 0.0017\t D_g_z2: 0.0063\t\n",
            "epcoh: 000017\t D_loss: 0.0152\t G_loss: 5.8424\t D_x: 0.2488\t D_g_z1: 0.0097\t D_g_z2: 0.0265\t\n",
            "epcoh: 000018\t D_loss: 0.0874\t G_loss: 5.3027\t D_x: 0.2392\t D_g_z1: 0.0363\t D_g_z2: 0.0903\t\n",
            "epcoh: 000019\t D_loss: 0.2418\t G_loss: 3.5906\t D_x: 0.2169\t D_g_z1: 0.0780\t D_g_z2: 0.2196\t\n",
            "epcoh: 000020\t D_loss: 0.2329\t G_loss: 3.1383\t D_x: 0.2175\t D_g_z1: 0.0787\t D_g_z2: 0.2374\t\n",
            "epcoh: 000021\t D_loss: 0.2549\t G_loss: 3.0069\t D_x: 0.2155\t D_g_z1: 0.0890\t D_g_z2: 0.2488\t\n",
            "epcoh: 000022\t D_loss: 0.2107\t G_loss: 3.2618\t D_x: 0.2231\t D_g_z1: 0.0754\t D_g_z2: 0.1921\t\n",
            "epcoh: 000023\t D_loss: 0.1765\t G_loss: 3.7192\t D_x: 0.2279\t D_g_z1: 0.0660\t D_g_z2: 0.1403\t\n",
            "epcoh: 000024\t D_loss: 0.1521\t G_loss: 4.0504\t D_x: 0.2327\t D_g_z1: 0.0583\t D_g_z2: 0.1059\t\n",
            "epcoh: 000025\t D_loss: 0.1178\t G_loss: 4.2345\t D_x: 0.2365\t D_g_z1: 0.0509\t D_g_z2: 0.0880\t\n",
            "epcoh: 000026\t D_loss: 0.0951\t G_loss: 4.5300\t D_x: 0.2402\t D_g_z1: 0.0447\t D_g_z2: 0.0709\t\n",
            "epcoh: 000027\t D_loss: 0.0876\t G_loss: 4.6335\t D_x: 0.2413\t D_g_z1: 0.0406\t D_g_z2: 0.0747\t\n",
            "epcoh: 000028\t D_loss: 0.0587\t G_loss: 4.6320\t D_x: 0.2440\t D_g_z1: 0.0299\t D_g_z2: 0.0561\t\n",
            "epcoh: 000029\t D_loss: 0.0738\t G_loss: 4.9224\t D_x: 0.2430\t D_g_z1: 0.0343\t D_g_z2: 0.0526\t\n",
            "epcoh: 000030\t D_loss: 0.0536\t G_loss: 4.7228\t D_x: 0.2453\t D_g_z1: 0.0271\t D_g_z2: 0.0541\t\n",
            "epcoh: 000031\t D_loss: 0.0640\t G_loss: 4.8483\t D_x: 0.2453\t D_g_z1: 0.0317\t D_g_z2: 0.0592\t\n",
            "epcoh: 000032\t D_loss: 0.0555\t G_loss: 5.0354\t D_x: 0.2456\t D_g_z1: 0.0275\t D_g_z2: 0.0438\t\n",
            "epcoh: 000033\t D_loss: 0.0445\t G_loss: 5.0787\t D_x: 0.2465\t D_g_z1: 0.0215\t D_g_z2: 0.0410\t\n",
            "epcoh: 000034\t D_loss: 0.0435\t G_loss: 5.3654\t D_x: 0.2465\t D_g_z1: 0.0216\t D_g_z2: 0.0417\t\n",
            "epcoh: 000035\t D_loss: 0.0452\t G_loss: 5.3087\t D_x: 0.2463\t D_g_z1: 0.0228\t D_g_z2: 0.0412\t\n",
            "epcoh: 000036\t D_loss: 0.0399\t G_loss: 5.3084\t D_x: 0.2470\t D_g_z1: 0.0201\t D_g_z2: 0.0376\t\n",
            "epcoh: 000037\t D_loss: 0.0406\t G_loss: 5.3794\t D_x: 0.2469\t D_g_z1: 0.0193\t D_g_z2: 0.0336\t\n",
            "epcoh: 000038\t D_loss: 0.0407\t G_loss: 5.4101\t D_x: 0.2468\t D_g_z1: 0.0198\t D_g_z2: 0.0358\t\n",
            "epcoh: 000039\t D_loss: 0.0376\t G_loss: 5.6564\t D_x: 0.2470\t D_g_z1: 0.0193\t D_g_z2: 0.0343\t\n",
            "epcoh: 000040\t D_loss: 0.0453\t G_loss: 5.8788\t D_x: 0.2461\t D_g_z1: 0.0208\t D_g_z2: 0.0293\t\n",
            "epcoh: 000041\t D_loss: 0.0349\t G_loss: 6.1504\t D_x: 0.2471\t D_g_z1: 0.0172\t D_g_z2: 0.0252\t\n",
            "epcoh: 000042\t D_loss: 0.0370\t G_loss: 6.1947\t D_x: 0.2471\t D_g_z1: 0.0165\t D_g_z2: 0.0283\t\n",
            "epcoh: 000043\t D_loss: 0.0364\t G_loss: 6.3679\t D_x: 0.2471\t D_g_z1: 0.0169\t D_g_z2: 0.0226\t\n",
            "epcoh: 000044\t D_loss: 0.0298\t G_loss: 6.3950\t D_x: 0.2477\t D_g_z1: 0.0144\t D_g_z2: 0.0188\t\n",
            "epcoh: 000045\t D_loss: 0.0318\t G_loss: 6.3018\t D_x: 0.2476\t D_g_z1: 0.0156\t D_g_z2: 0.0203\t\n",
            "epcoh: 000046\t D_loss: 0.0326\t G_loss: 6.0381\t D_x: 0.2475\t D_g_z1: 0.0166\t D_g_z2: 0.0279\t\n",
            "epcoh: 000047\t D_loss: 0.0243\t G_loss: 6.0421\t D_x: 0.2480\t D_g_z1: 0.0132\t D_g_z2: 0.0277\t\n",
            "epcoh: 000048\t D_loss: 0.0379\t G_loss: 6.4674\t D_x: 0.2469\t D_g_z1: 0.0187\t D_g_z2: 0.0302\t\n",
            "epcoh: 000049\t D_loss: 0.0441\t G_loss: 6.5767\t D_x: 0.2469\t D_g_z1: 0.0202\t D_g_z2: 0.0237\t\n",
            "epcoh: 000050\t D_loss: 0.0255\t G_loss: 6.2268\t D_x: 0.2479\t D_g_z1: 0.0139\t D_g_z2: 0.0217\t\n",
            "epcoh: 000051\t D_loss: 0.0269\t G_loss: 6.5670\t D_x: 0.2481\t D_g_z1: 0.0137\t D_g_z2: 0.0275\t\n",
            "epcoh: 000052\t D_loss: 0.0285\t G_loss: 6.7589\t D_x: 0.2479\t D_g_z1: 0.0135\t D_g_z2: 0.0178\t\n",
            "epcoh: 000053\t D_loss: 0.0267\t G_loss: 6.7144\t D_x: 0.2477\t D_g_z1: 0.0122\t D_g_z2: 0.0216\t\n",
            "epcoh: 000054\t D_loss: 0.0257\t G_loss: 6.8363\t D_x: 0.2482\t D_g_z1: 0.0131\t D_g_z2: 0.0224\t\n",
            "epcoh: 000055\t D_loss: 0.0388\t G_loss: 7.2536\t D_x: 0.2480\t D_g_z1: 0.0166\t D_g_z2: 0.0283\t\n",
            "epcoh: 000056\t D_loss: 0.0328\t G_loss: 6.4229\t D_x: 0.2480\t D_g_z1: 0.0162\t D_g_z2: 0.0385\t\n",
            "epcoh: 000057\t D_loss: 0.0306\t G_loss: 6.3773\t D_x: 0.2482\t D_g_z1: 0.0152\t D_g_z2: 0.0305\t\n",
            "epcoh: 000058\t D_loss: 0.0297\t G_loss: 6.4669\t D_x: 0.2476\t D_g_z1: 0.0143\t D_g_z2: 0.0283\t\n",
            "epcoh: 000059\t D_loss: 0.0323\t G_loss: 6.6669\t D_x: 0.2481\t D_g_z1: 0.0156\t D_g_z2: 0.0344\t\n",
            "epcoh: 000060\t D_loss: 0.0274\t G_loss: 5.7435\t D_x: 0.2483\t D_g_z1: 0.0152\t D_g_z2: 0.0454\t\n",
            "epcoh: 000061\t D_loss: 0.0398\t G_loss: 5.7119\t D_x: 0.2476\t D_g_z1: 0.0213\t D_g_z2: 0.0371\t\n",
            "epcoh: 000062\t D_loss: 0.0318\t G_loss: 5.2886\t D_x: 0.2482\t D_g_z1: 0.0176\t D_g_z2: 0.0507\t\n",
            "epcoh: 000063\t D_loss: 0.0372\t G_loss: 5.8062\t D_x: 0.2481\t D_g_z1: 0.0194\t D_g_z2: 0.0352\t\n",
            "epcoh: 000064\t D_loss: 0.0479\t G_loss: 5.3466\t D_x: 0.2478\t D_g_z1: 0.0281\t D_g_z2: 0.0644\t\n",
            "epcoh: 000065\t D_loss: 0.0395\t G_loss: 4.5827\t D_x: 0.2484\t D_g_z1: 0.0270\t D_g_z2: 0.0792\t\n",
            "epcoh: 000066\t D_loss: 0.0775\t G_loss: 3.8245\t D_x: 0.2478\t D_g_z1: 0.0534\t D_g_z2: 0.1482\t\n",
            "epcoh: 000067\t D_loss: 0.1198\t G_loss: 3.3148\t D_x: 0.2473\t D_g_z1: 0.0846\t D_g_z2: 0.2516\t\n",
            "epcoh: 000068\t D_loss: 0.0738\t G_loss: 3.5568\t D_x: 0.2479\t D_g_z1: 0.0514\t D_g_z2: 0.1781\t\n",
            "epcoh: 000069\t D_loss: 0.0442\t G_loss: 4.2940\t D_x: 0.2481\t D_g_z1: 0.0282\t D_g_z2: 0.0908\t\n",
            "epcoh: 000070\t D_loss: 0.0303\t G_loss: 5.1713\t D_x: 0.2488\t D_g_z1: 0.0186\t D_g_z2: 0.0532\t\n",
            "epcoh: 000071\t D_loss: 0.0427\t G_loss: 5.7766\t D_x: 0.2475\t D_g_z1: 0.0185\t D_g_z2: 0.0404\t\n",
            "epcoh: 000072\t D_loss: 0.0573\t G_loss: 6.2414\t D_x: 0.2467\t D_g_z1: 0.0213\t D_g_z2: 0.0369\t\n",
            "epcoh: 000073\t D_loss: 0.0854\t G_loss: 7.3466\t D_x: 0.2453\t D_g_z1: 0.0205\t D_g_z2: 0.0313\t\n",
            "epcoh: 000074\t D_loss: 0.0602\t G_loss: 7.5485\t D_x: 0.2458\t D_g_z1: 0.0161\t D_g_z2: 0.0219\t\n",
            "epcoh: 000075\t D_loss: 0.0604\t G_loss: 8.2768\t D_x: 0.2448\t D_g_z1: 0.0170\t D_g_z2: 0.0217\t\n",
            "epcoh: 000076\t D_loss: 0.0572\t G_loss: 9.1901\t D_x: 0.2459\t D_g_z1: 0.0146\t D_g_z2: 0.0352\t\n",
            "epcoh: 000077\t D_loss: 0.0503\t G_loss: 8.6300\t D_x: 0.2458\t D_g_z1: 0.0121\t D_g_z2: 0.0160\t\n",
            "epcoh: 000078\t D_loss: 0.0566\t G_loss: 9.7509\t D_x: 0.2457\t D_g_z1: 0.0127\t D_g_z2: 0.0262\t\n",
            "epcoh: 000079\t D_loss: 0.0574\t G_loss: 8.9468\t D_x: 0.2456\t D_g_z1: 0.0131\t D_g_z2: 0.0420\t\n",
            "epcoh: 000080\t D_loss: 0.0520\t G_loss: 9.5328\t D_x: 0.2457\t D_g_z1: 0.0135\t D_g_z2: 0.0231\t\n",
            "epcoh: 000081\t D_loss: 0.0377\t G_loss: 9.1706\t D_x: 0.2468\t D_g_z1: 0.0104\t D_g_z2: 0.0212\t\n",
            "epcoh: 000082\t D_loss: 0.0350\t G_loss: 8.6572\t D_x: 0.2472\t D_g_z1: 0.0100\t D_g_z2: 0.0204\t\n",
            "epcoh: 000083\t D_loss: 0.0391\t G_loss: 8.7254\t D_x: 0.2466\t D_g_z1: 0.0096\t D_g_z2: 0.0144\t\n",
            "epcoh: 000084\t D_loss: 0.0223\t G_loss: 8.1038\t D_x: 0.2479\t D_g_z1: 0.0069\t D_g_z2: 0.0120\t\n",
            "epcoh: 000085\t D_loss: 0.0346\t G_loss: 9.4130\t D_x: 0.2473\t D_g_z1: 0.0091\t D_g_z2: 0.0202\t\n",
            "epcoh: 000086\t D_loss: 0.0408\t G_loss: 8.4785\t D_x: 0.2466\t D_g_z1: 0.0115\t D_g_z2: 0.0192\t\n",
            "epcoh: 000087\t D_loss: 0.0280\t G_loss: 8.1331\t D_x: 0.2477\t D_g_z1: 0.0074\t D_g_z2: 0.0175\t\n",
            "epcoh: 000088\t D_loss: 0.0218\t G_loss: 7.7535\t D_x: 0.2480\t D_g_z1: 0.0076\t D_g_z2: 0.0173\t\n",
            "epcoh: 000089\t D_loss: 0.0192\t G_loss: 7.3337\t D_x: 0.2482\t D_g_z1: 0.0064\t D_g_z2: 0.0160\t\n",
            "epcoh: 000090\t D_loss: 0.0360\t G_loss: 7.8586\t D_x: 0.2474\t D_g_z1: 0.0105\t D_g_z2: 0.0271\t\n",
            "epcoh: 000091\t D_loss: 0.0254\t G_loss: 8.0512\t D_x: 0.2479\t D_g_z1: 0.0074\t D_g_z2: 0.0162\t\n",
            "epcoh: 000092\t D_loss: 0.0208\t G_loss: 7.4040\t D_x: 0.2481\t D_g_z1: 0.0072\t D_g_z2: 0.0233\t\n",
            "epcoh: 000093\t D_loss: 0.0214\t G_loss: 7.5861\t D_x: 0.2483\t D_g_z1: 0.0071\t D_g_z2: 0.0209\t\n",
            "epcoh: 000094\t D_loss: 0.0189\t G_loss: 7.5867\t D_x: 0.2482\t D_g_z1: 0.0075\t D_g_z2: 0.0162\t\n",
            "epcoh: 000095\t D_loss: 0.0257\t G_loss: 7.5860\t D_x: 0.2482\t D_g_z1: 0.0079\t D_g_z2: 0.0203\t\n",
            "epcoh: 000096\t D_loss: 0.0254\t G_loss: 7.9174\t D_x: 0.2479\t D_g_z1: 0.0087\t D_g_z2: 0.0145\t\n",
            "epcoh: 000097\t D_loss: 0.0226\t G_loss: 7.3375\t D_x: 0.2481\t D_g_z1: 0.0066\t D_g_z2: 0.0162\t\n",
            "epcoh: 000098\t D_loss: 0.0182\t G_loss: 7.7017\t D_x: 0.2484\t D_g_z1: 0.0059\t D_g_z2: 0.0130\t\n",
            "epcoh: 000099\t D_loss: 0.0182\t G_loss: 8.6408\t D_x: 0.2485\t D_g_z1: 0.0063\t D_g_z2: 0.0092\t\n",
            "epcoh: 000100\t D_loss: 0.0176\t G_loss: 7.7555\t D_x: 0.2483\t D_g_z1: 0.0069\t D_g_z2: 0.0216\t\n",
            "epcoh: 000101\t D_loss: 0.0313\t G_loss: 7.5389\t D_x: 0.2475\t D_g_z1: 0.0103\t D_g_z2: 0.0223\t\n",
            "epcoh: 000102\t D_loss: 0.0232\t G_loss: 7.7381\t D_x: 0.2483\t D_g_z1: 0.0082\t D_g_z2: 0.0179\t\n",
            "epcoh: 000103\t D_loss: 0.0251\t G_loss: 7.3186\t D_x: 0.2483\t D_g_z1: 0.0076\t D_g_z2: 0.0209\t\n",
            "epcoh: 000104\t D_loss: 0.0209\t G_loss: 7.9704\t D_x: 0.2485\t D_g_z1: 0.0080\t D_g_z2: 0.0222\t\n",
            "epcoh: 000105\t D_loss: 0.0171\t G_loss: 7.8496\t D_x: 0.2490\t D_g_z1: 0.0061\t D_g_z2: 0.0113\t\n",
            "epcoh: 000106\t D_loss: 0.0289\t G_loss: 8.1977\t D_x: 0.2484\t D_g_z1: 0.0097\t D_g_z2: 0.0239\t\n",
            "epcoh: 000107\t D_loss: 0.0212\t G_loss: 7.6504\t D_x: 0.2483\t D_g_z1: 0.0079\t D_g_z2: 0.0157\t\n",
            "epcoh: 000108\t D_loss: 0.0271\t G_loss: 7.2551\t D_x: 0.2483\t D_g_z1: 0.0103\t D_g_z2: 0.0267\t\n",
            "epcoh: 000109\t D_loss: 0.0378\t G_loss: 7.8099\t D_x: 0.2477\t D_g_z1: 0.0119\t D_g_z2: 0.0343\t\n",
            "epcoh: 000110\t D_loss: 0.0264\t G_loss: 7.8107\t D_x: 0.2485\t D_g_z1: 0.0092\t D_g_z2: 0.0220\t\n",
            "epcoh: 000111\t D_loss: 0.0263\t G_loss: 7.5875\t D_x: 0.2484\t D_g_z1: 0.0095\t D_g_z2: 0.0204\t\n",
            "epcoh: 000112\t D_loss: 0.0309\t G_loss: 7.5984\t D_x: 0.2479\t D_g_z1: 0.0106\t D_g_z2: 0.0227\t\n",
            "epcoh: 000113\t D_loss: 0.0215\t G_loss: 7.7998\t D_x: 0.2486\t D_g_z1: 0.0090\t D_g_z2: 0.0271\t\n",
            "epcoh: 000114\t D_loss: 0.0319\t G_loss: 8.3028\t D_x: 0.2484\t D_g_z1: 0.0111\t D_g_z2: 0.0153\t\n",
            "epcoh: 000115\t D_loss: 0.0164\t G_loss: 8.1639\t D_x: 0.2493\t D_g_z1: 0.0051\t D_g_z2: 0.0125\t\n",
            "epcoh: 000116\t D_loss: 0.0136\t G_loss: 7.8666\t D_x: 0.2491\t D_g_z1: 0.0055\t D_g_z2: 0.0104\t\n",
            "epcoh: 000117\t D_loss: 0.0174\t G_loss: 8.1874\t D_x: 0.2489\t D_g_z1: 0.0069\t D_g_z2: 0.0130\t\n",
            "epcoh: 000118\t D_loss: 0.0153\t G_loss: 8.0234\t D_x: 0.2492\t D_g_z1: 0.0059\t D_g_z2: 0.0101\t\n",
            "epcoh: 000119\t D_loss: 0.0107\t G_loss: 7.8069\t D_x: 0.2493\t D_g_z1: 0.0047\t D_g_z2: 0.0127\t\n",
            "epcoh: 000120\t D_loss: 0.0143\t G_loss: 7.9528\t D_x: 0.2493\t D_g_z1: 0.0053\t D_g_z2: 0.0160\t\n",
            "epcoh: 000121\t D_loss: 0.0112\t G_loss: 8.5216\t D_x: 0.2493\t D_g_z1: 0.0042\t D_g_z2: 0.0090\t\n",
            "epcoh: 000122\t D_loss: 0.0106\t G_loss: 8.9672\t D_x: 0.2494\t D_g_z1: 0.0045\t D_g_z2: 0.0081\t\n",
            "epcoh: 000123\t D_loss: 0.0136\t G_loss: 8.2081\t D_x: 0.2490\t D_g_z1: 0.0056\t D_g_z2: 0.0160\t\n",
            "epcoh: 000124\t D_loss: 0.0156\t G_loss: 8.1702\t D_x: 0.2491\t D_g_z1: 0.0065\t D_g_z2: 0.0147\t\n",
            "epcoh: 000125\t D_loss: 0.0084\t G_loss: 8.5715\t D_x: 0.2494\t D_g_z1: 0.0039\t D_g_z2: 0.0083\t\n",
            "epcoh: 000126\t D_loss: 0.0124\t G_loss: 7.9875\t D_x: 0.2495\t D_g_z1: 0.0043\t D_g_z2: 0.0098\t\n",
            "epcoh: 000127\t D_loss: 0.0087\t G_loss: 8.5281\t D_x: 0.2495\t D_g_z1: 0.0037\t D_g_z2: 0.0145\t\n",
            "epcoh: 000128\t D_loss: 0.0084\t G_loss: 8.5593\t D_x: 0.2494\t D_g_z1: 0.0036\t D_g_z2: 0.0186\t\n",
            "epcoh: 000129\t D_loss: 0.0078\t G_loss: 7.8513\t D_x: 0.2496\t D_g_z1: 0.0034\t D_g_z2: 0.0055\t\n",
            "epcoh: 000130\t D_loss: 0.0076\t G_loss: 7.7280\t D_x: 0.2495\t D_g_z1: 0.0032\t D_g_z2: 0.0112\t\n",
            "epcoh: 000131\t D_loss: 0.0079\t G_loss: 7.7395\t D_x: 0.2496\t D_g_z1: 0.0031\t D_g_z2: 0.0111\t\n",
            "epcoh: 000132\t D_loss: 0.0113\t G_loss: 8.4101\t D_x: 0.2495\t D_g_z1: 0.0034\t D_g_z2: 0.0136\t\n",
            "epcoh: 000133\t D_loss: 0.0140\t G_loss: 9.0828\t D_x: 0.2495\t D_g_z1: 0.0046\t D_g_z2: 0.0105\t\n",
            "epcoh: 000134\t D_loss: 0.0191\t G_loss: 9.3434\t D_x: 0.2494\t D_g_z1: 0.0069\t D_g_z2: 0.0159\t\n",
            "epcoh: 000135\t D_loss: 0.0248\t G_loss: 9.1285\t D_x: 0.2488\t D_g_z1: 0.0082\t D_g_z2: 0.0147\t\n",
            "epcoh: 000136\t D_loss: 0.0273\t G_loss: 7.7426\t D_x: 0.2487\t D_g_z1: 0.0111\t D_g_z2: 0.0240\t\n",
            "epcoh: 000137\t D_loss: 0.0172\t G_loss: 8.3993\t D_x: 0.2489\t D_g_z1: 0.0074\t D_g_z2: 0.0153\t\n",
            "epcoh: 000138\t D_loss: 0.0212\t G_loss: 8.0631\t D_x: 0.2493\t D_g_z1: 0.0084\t D_g_z2: 0.0156\t\n",
            "epcoh: 000139\t D_loss: 0.0264\t G_loss: 8.3559\t D_x: 0.2488\t D_g_z1: 0.0101\t D_g_z2: 0.0162\t\n",
            "epcoh: 000140\t D_loss: 0.0331\t G_loss: 7.8941\t D_x: 0.2482\t D_g_z1: 0.0121\t D_g_z2: 0.0308\t\n",
            "epcoh: 000141\t D_loss: 0.0351\t G_loss: 8.0746\t D_x: 0.2481\t D_g_z1: 0.0140\t D_g_z2: 0.0336\t\n",
            "epcoh: 000142\t D_loss: 0.0368\t G_loss: 7.2342\t D_x: 0.2481\t D_g_z1: 0.0151\t D_g_z2: 0.0300\t\n",
            "epcoh: 000143\t D_loss: 0.0390\t G_loss: 7.8152\t D_x: 0.2484\t D_g_z1: 0.0129\t D_g_z2: 0.0246\t\n",
            "epcoh: 000144\t D_loss: 0.0389\t G_loss: 7.9444\t D_x: 0.2482\t D_g_z1: 0.0145\t D_g_z2: 0.0305\t\n",
            "epcoh: 000145\t D_loss: 0.0323\t G_loss: 8.0863\t D_x: 0.2483\t D_g_z1: 0.0127\t D_g_z2: 0.0267\t\n",
            "epcoh: 000146\t D_loss: 0.0279\t G_loss: 7.7500\t D_x: 0.2487\t D_g_z1: 0.0097\t D_g_z2: 0.0308\t\n",
            "epcoh: 000147\t D_loss: 0.0429\t G_loss: 7.8946\t D_x: 0.2476\t D_g_z1: 0.0133\t D_g_z2: 0.0454\t\n",
            "epcoh: 000148\t D_loss: 0.0239\t G_loss: 8.7103\t D_x: 0.2487\t D_g_z1: 0.0094\t D_g_z2: 0.0292\t\n",
            "epcoh: 000149\t D_loss: 0.0260\t G_loss: 8.5412\t D_x: 0.2489\t D_g_z1: 0.0082\t D_g_z2: 0.0267\t\n",
            "epcoh: 000150\t D_loss: 0.0137\t G_loss: 9.6191\t D_x: 0.2493\t D_g_z1: 0.0070\t D_g_z2: 0.0199\t\n",
            "epcoh: 000151\t D_loss: 0.0228\t G_loss: 8.5460\t D_x: 0.2489\t D_g_z1: 0.0080\t D_g_z2: 0.0208\t\n",
            "epcoh: 000152\t D_loss: 0.0158\t G_loss: 8.0053\t D_x: 0.2495\t D_g_z1: 0.0069\t D_g_z2: 0.0102\t\n",
            "epcoh: 000153\t D_loss: 0.0190\t G_loss: 8.4927\t D_x: 0.2491\t D_g_z1: 0.0067\t D_g_z2: 0.0283\t\n",
            "epcoh: 000154\t D_loss: 0.0182\t G_loss: 8.6062\t D_x: 0.2492\t D_g_z1: 0.0057\t D_g_z2: 0.0194\t\n",
            "epcoh: 000155\t D_loss: 0.0147\t G_loss: 7.9517\t D_x: 0.2493\t D_g_z1: 0.0054\t D_g_z2: 0.0200\t\n",
            "epcoh: 000156\t D_loss: 0.0136\t G_loss: 9.0289\t D_x: 0.2494\t D_g_z1: 0.0053\t D_g_z2: 0.0128\t\n",
            "epcoh: 000157\t D_loss: 0.0119\t G_loss: 8.3257\t D_x: 0.2491\t D_g_z1: 0.0055\t D_g_z2: 0.0154\t\n",
            "epcoh: 000158\t D_loss: 0.0107\t G_loss: 7.7877\t D_x: 0.2494\t D_g_z1: 0.0048\t D_g_z2: 0.0162\t\n",
            "epcoh: 000159\t D_loss: 0.0157\t G_loss: 8.0538\t D_x: 0.2490\t D_g_z1: 0.0063\t D_g_z2: 0.0148\t\n",
            "epcoh: 000160\t D_loss: 0.0202\t G_loss: 7.9495\t D_x: 0.2490\t D_g_z1: 0.0076\t D_g_z2: 0.0197\t\n",
            "epcoh: 000161\t D_loss: 0.0154\t G_loss: 8.0074\t D_x: 0.2493\t D_g_z1: 0.0064\t D_g_z2: 0.0177\t\n",
            "epcoh: 000162\t D_loss: 0.0159\t G_loss: 8.7001\t D_x: 0.2491\t D_g_z1: 0.0060\t D_g_z2: 0.0189\t\n",
            "epcoh: 000163\t D_loss: 0.0121\t G_loss: 8.0625\t D_x: 0.2493\t D_g_z1: 0.0058\t D_g_z2: 0.0092\t\n",
            "epcoh: 000164\t D_loss: 0.0095\t G_loss: 7.7074\t D_x: 0.2493\t D_g_z1: 0.0045\t D_g_z2: 0.0136\t\n",
            "epcoh: 000165\t D_loss: 0.0113\t G_loss: 7.6836\t D_x: 0.2493\t D_g_z1: 0.0056\t D_g_z2: 0.0125\t\n",
            "epcoh: 000166\t D_loss: 0.0100\t G_loss: 7.5762\t D_x: 0.2494\t D_g_z1: 0.0041\t D_g_z2: 0.0123\t\n",
            "epcoh: 000167\t D_loss: 0.0101\t G_loss: 7.4199\t D_x: 0.2493\t D_g_z1: 0.0044\t D_g_z2: 0.0119\t\n",
            "epcoh: 000168\t D_loss: 0.0051\t G_loss: 7.9503\t D_x: 0.2497\t D_g_z1: 0.0026\t D_g_z2: 0.0072\t\n",
            "epcoh: 000169\t D_loss: 0.0042\t G_loss: 7.7014\t D_x: 0.2497\t D_g_z1: 0.0019\t D_g_z2: 0.0067\t\n",
            "epcoh: 000170\t D_loss: 0.0035\t G_loss: 8.0698\t D_x: 0.2498\t D_g_z1: 0.0019\t D_g_z2: 0.0056\t\n",
            "epcoh: 000171\t D_loss: 0.0052\t G_loss: 8.1622\t D_x: 0.2497\t D_g_z1: 0.0026\t D_g_z2: 0.0093\t\n",
            "epcoh: 000172\t D_loss: 0.0052\t G_loss: 8.6323\t D_x: 0.2497\t D_g_z1: 0.0028\t D_g_z2: 0.0117\t\n",
            "epcoh: 000173\t D_loss: 0.0048\t G_loss: 8.3945\t D_x: 0.2496\t D_g_z1: 0.0019\t D_g_z2: 0.0055\t\n",
            "epcoh: 000174\t D_loss: 0.0049\t G_loss: 8.1321\t D_x: 0.2497\t D_g_z1: 0.0023\t D_g_z2: 0.0059\t\n",
            "epcoh: 000175\t D_loss: 0.0070\t G_loss: 8.2070\t D_x: 0.2497\t D_g_z1: 0.0028\t D_g_z2: 0.0100\t\n",
            "epcoh: 000176\t D_loss: 0.0053\t G_loss: 7.7844\t D_x: 0.2496\t D_g_z1: 0.0025\t D_g_z2: 0.0099\t\n",
            "epcoh: 000177\t D_loss: 0.0091\t G_loss: 7.7754\t D_x: 0.2495\t D_g_z1: 0.0036\t D_g_z2: 0.0087\t\n",
            "epcoh: 000178\t D_loss: 0.0080\t G_loss: 8.2683\t D_x: 0.2496\t D_g_z1: 0.0021\t D_g_z2: 0.0092\t\n",
            "epcoh: 000179\t D_loss: 0.0092\t G_loss: 7.7136\t D_x: 0.2495\t D_g_z1: 0.0037\t D_g_z2: 0.0100\t\n",
            "epcoh: 000180\t D_loss: 0.0053\t G_loss: 7.9672\t D_x: 0.2497\t D_g_z1: 0.0028\t D_g_z2: 0.0088\t\n",
            "epcoh: 000181\t D_loss: 0.0114\t G_loss: 7.9480\t D_x: 0.2494\t D_g_z1: 0.0041\t D_g_z2: 0.0128\t\n",
            "epcoh: 000182\t D_loss: 0.0197\t G_loss: 8.2045\t D_x: 0.2491\t D_g_z1: 0.0078\t D_g_z2: 0.0187\t\n",
            "epcoh: 000183\t D_loss: 0.0151\t G_loss: 8.0350\t D_x: 0.2494\t D_g_z1: 0.0051\t D_g_z2: 0.0157\t\n",
            "epcoh: 000184\t D_loss: 0.0065\t G_loss: 8.0872\t D_x: 0.2496\t D_g_z1: 0.0030\t D_g_z2: 0.0066\t\n",
            "epcoh: 000185\t D_loss: 0.0070\t G_loss: 8.0385\t D_x: 0.2496\t D_g_z1: 0.0029\t D_g_z2: 0.0060\t\n",
            "epcoh: 000186\t D_loss: 0.0163\t G_loss: 7.9946\t D_x: 0.2492\t D_g_z1: 0.0055\t D_g_z2: 0.0134\t\n",
            "epcoh: 000187\t D_loss: 0.0131\t G_loss: 8.6044\t D_x: 0.2493\t D_g_z1: 0.0049\t D_g_z2: 0.0092\t\n",
            "epcoh: 000188\t D_loss: 0.0159\t G_loss: 8.8429\t D_x: 0.2494\t D_g_z1: 0.0055\t D_g_z2: 0.0124\t\n",
            "epcoh: 000189\t D_loss: 0.0138\t G_loss: 8.6278\t D_x: 0.2495\t D_g_z1: 0.0047\t D_g_z2: 0.0071\t\n",
            "epcoh: 000190\t D_loss: 0.0162\t G_loss: 8.4126\t D_x: 0.2493\t D_g_z1: 0.0049\t D_g_z2: 0.0209\t\n",
            "epcoh: 000191\t D_loss: 0.0196\t G_loss: 7.7351\t D_x: 0.2489\t D_g_z1: 0.0056\t D_g_z2: 0.0205\t\n",
            "epcoh: 000192\t D_loss: 0.0243\t G_loss: 7.8026\t D_x: 0.2491\t D_g_z1: 0.0048\t D_g_z2: 0.0164\t\n",
            "epcoh: 000193\t D_loss: 0.0171\t G_loss: 8.4765\t D_x: 0.2493\t D_g_z1: 0.0053\t D_g_z2: 0.0182\t\n",
            "epcoh: 000194\t D_loss: 0.0200\t G_loss: 8.2139\t D_x: 0.2488\t D_g_z1: 0.0060\t D_g_z2: 0.0116\t\n",
            "epcoh: 000195\t D_loss: 0.0249\t G_loss: 7.9895\t D_x: 0.2489\t D_g_z1: 0.0066\t D_g_z2: 0.0152\t\n",
            "epcoh: 000196\t D_loss: 0.0197\t G_loss: 8.0529\t D_x: 0.2491\t D_g_z1: 0.0055\t D_g_z2: 0.0150\t\n",
            "epcoh: 000197\t D_loss: 0.0083\t G_loss: 8.6963\t D_x: 0.2495\t D_g_z1: 0.0039\t D_g_z2: 0.0079\t\n",
            "epcoh: 000198\t D_loss: 0.0195\t G_loss: 8.8471\t D_x: 0.2491\t D_g_z1: 0.0052\t D_g_z2: 0.0086\t\n",
            "epcoh: 000199\t D_loss: 0.0141\t G_loss: 8.7022\t D_x: 0.2493\t D_g_z1: 0.0048\t D_g_z2: 0.0161\t\n",
            "epcoh: 000200\t D_loss: 0.0144\t G_loss: 8.5907\t D_x: 0.2491\t D_g_z1: 0.0050\t D_g_z2: 0.0094\t\n",
            "epcoh: 000201\t D_loss: 0.0189\t G_loss: 9.1424\t D_x: 0.2492\t D_g_z1: 0.0065\t D_g_z2: 0.0108\t\n",
            "epcoh: 000202\t D_loss: 0.0275\t G_loss: 8.7228\t D_x: 0.2491\t D_g_z1: 0.0079\t D_g_z2: 0.0255\t\n",
            "epcoh: 000203\t D_loss: 0.0209\t G_loss: 8.9233\t D_x: 0.2493\t D_g_z1: 0.0063\t D_g_z2: 0.0210\t\n",
            "epcoh: 000204\t D_loss: 0.0141\t G_loss: 8.0820\t D_x: 0.2495\t D_g_z1: 0.0056\t D_g_z2: 0.0149\t\n",
            "epcoh: 000205\t D_loss: 0.0203\t G_loss: 8.1130\t D_x: 0.2495\t D_g_z1: 0.0059\t D_g_z2: 0.0132\t\n",
            "epcoh: 000206\t D_loss: 0.0212\t G_loss: 8.3702\t D_x: 0.2494\t D_g_z1: 0.0058\t D_g_z2: 0.0237\t\n",
            "epcoh: 000207\t D_loss: 0.0135\t G_loss: 9.0965\t D_x: 0.2494\t D_g_z1: 0.0053\t D_g_z2: 0.0180\t\n",
            "epcoh: 000208\t D_loss: 0.0084\t G_loss: 8.7025\t D_x: 0.2496\t D_g_z1: 0.0033\t D_g_z2: 0.0155\t\n",
            "epcoh: 000209\t D_loss: 0.0158\t G_loss: 8.1587\t D_x: 0.2495\t D_g_z1: 0.0050\t D_g_z2: 0.0100\t\n",
            "epcoh: 000210\t D_loss: 0.0125\t G_loss: 8.4984\t D_x: 0.2495\t D_g_z1: 0.0050\t D_g_z2: 0.0082\t\n",
            "epcoh: 000211\t D_loss: 0.0102\t G_loss: 8.0734\t D_x: 0.2496\t D_g_z1: 0.0039\t D_g_z2: 0.0209\t\n",
            "epcoh: 000212\t D_loss: 0.0101\t G_loss: 8.1554\t D_x: 0.2496\t D_g_z1: 0.0039\t D_g_z2: 0.0126\t\n",
            "epcoh: 000213\t D_loss: 0.0094\t G_loss: 8.7696\t D_x: 0.2496\t D_g_z1: 0.0041\t D_g_z2: 0.0130\t\n",
            "epcoh: 000214\t D_loss: 0.0155\t G_loss: 8.2925\t D_x: 0.2493\t D_g_z1: 0.0056\t D_g_z2: 0.0182\t\n",
            "epcoh: 000215\t D_loss: 0.0181\t G_loss: 8.6555\t D_x: 0.2493\t D_g_z1: 0.0064\t D_g_z2: 0.0133\t\n",
            "epcoh: 000216\t D_loss: 0.0175\t G_loss: 8.2567\t D_x: 0.2493\t D_g_z1: 0.0057\t D_g_z2: 0.0198\t\n",
            "epcoh: 000217\t D_loss: 0.0121\t G_loss: 8.1317\t D_x: 0.2494\t D_g_z1: 0.0057\t D_g_z2: 0.0138\t\n",
            "epcoh: 000218\t D_loss: 0.0213\t G_loss: 7.7877\t D_x: 0.2492\t D_g_z1: 0.0075\t D_g_z2: 0.0225\t\n",
            "epcoh: 000219\t D_loss: 0.0137\t G_loss: 7.8041\t D_x: 0.2494\t D_g_z1: 0.0055\t D_g_z2: 0.0298\t\n",
            "epcoh: 000220\t D_loss: 0.0137\t G_loss: 7.4947\t D_x: 0.2494\t D_g_z1: 0.0053\t D_g_z2: 0.0143\t\n",
            "epcoh: 000221\t D_loss: 0.0116\t G_loss: 7.6130\t D_x: 0.2495\t D_g_z1: 0.0042\t D_g_z2: 0.0142\t\n",
            "epcoh: 000222\t D_loss: 0.0103\t G_loss: 7.7462\t D_x: 0.2495\t D_g_z1: 0.0049\t D_g_z2: 0.0171\t\n",
            "epcoh: 000223\t D_loss: 0.0112\t G_loss: 7.8785\t D_x: 0.2494\t D_g_z1: 0.0049\t D_g_z2: 0.0171\t\n",
            "epcoh: 000224\t D_loss: 0.0151\t G_loss: 7.9988\t D_x: 0.2495\t D_g_z1: 0.0054\t D_g_z2: 0.0213\t\n",
            "epcoh: 000225\t D_loss: 0.0097\t G_loss: 8.0748\t D_x: 0.2496\t D_g_z1: 0.0043\t D_g_z2: 0.0122\t\n",
            "epcoh: 000226\t D_loss: 0.0110\t G_loss: 8.5481\t D_x: 0.2494\t D_g_z1: 0.0034\t D_g_z2: 0.0164\t\n",
            "epcoh: 000227\t D_loss: 0.0079\t G_loss: 8.2264\t D_x: 0.2496\t D_g_z1: 0.0035\t D_g_z2: 0.0111\t\n",
            "epcoh: 000228\t D_loss: 0.0125\t G_loss: 7.7234\t D_x: 0.2495\t D_g_z1: 0.0045\t D_g_z2: 0.0128\t\n",
            "epcoh: 000229\t D_loss: 0.0055\t G_loss: 7.9811\t D_x: 0.2497\t D_g_z1: 0.0028\t D_g_z2: 0.0106\t\n",
            "epcoh: 000230\t D_loss: 0.0127\t G_loss: 7.5839\t D_x: 0.2493\t D_g_z1: 0.0043\t D_g_z2: 0.0126\t\n",
            "epcoh: 000231\t D_loss: 0.0131\t G_loss: 7.8377\t D_x: 0.2495\t D_g_z1: 0.0042\t D_g_z2: 0.0130\t\n",
            "epcoh: 000232\t D_loss: 0.0079\t G_loss: 8.3528\t D_x: 0.2496\t D_g_z1: 0.0031\t D_g_z2: 0.0086\t\n",
            "epcoh: 000233\t D_loss: 0.0036\t G_loss: 8.4092\t D_x: 0.2497\t D_g_z1: 0.0018\t D_g_z2: 0.0053\t\n",
            "epcoh: 000234\t D_loss: 0.0063\t G_loss: 8.5375\t D_x: 0.2496\t D_g_z1: 0.0021\t D_g_z2: 0.0075\t\n",
            "epcoh: 000235\t D_loss: 0.0107\t G_loss: 8.2110\t D_x: 0.2495\t D_g_z1: 0.0032\t D_g_z2: 0.0060\t\n",
            "epcoh: 000236\t D_loss: 0.0072\t G_loss: 8.2044\t D_x: 0.2497\t D_g_z1: 0.0024\t D_g_z2: 0.0052\t\n",
            "epcoh: 000237\t D_loss: 0.0069\t G_loss: 8.1834\t D_x: 0.2496\t D_g_z1: 0.0029\t D_g_z2: 0.0075\t\n",
            "epcoh: 000238\t D_loss: 0.0067\t G_loss: 8.5577\t D_x: 0.2497\t D_g_z1: 0.0028\t D_g_z2: 0.0106\t\n",
            "epcoh: 000239\t D_loss: 0.0038\t G_loss: 8.4387\t D_x: 0.2497\t D_g_z1: 0.0019\t D_g_z2: 0.0084\t\n",
            "epcoh: 000240\t D_loss: 0.0039\t G_loss: 8.5041\t D_x: 0.2498\t D_g_z1: 0.0017\t D_g_z2: 0.0049\t\n",
            "epcoh: 000241\t D_loss: 0.0025\t G_loss: 8.8329\t D_x: 0.2498\t D_g_z1: 0.0010\t D_g_z2: 0.0021\t\n",
            "epcoh: 000242\t D_loss: 0.0068\t G_loss: 8.9942\t D_x: 0.2497\t D_g_z1: 0.0017\t D_g_z2: 0.0092\t\n",
            "epcoh: 000243\t D_loss: 0.0042\t G_loss: 9.2604\t D_x: 0.2497\t D_g_z1: 0.0017\t D_g_z2: 0.0026\t\n",
            "epcoh: 000244\t D_loss: 0.0035\t G_loss: 9.4337\t D_x: 0.2498\t D_g_z1: 0.0015\t D_g_z2: 0.0031\t\n",
            "epcoh: 000245\t D_loss: 0.0031\t G_loss: 10.0130\t D_x: 0.2498\t D_g_z1: 0.0015\t D_g_z2: 0.0027\t\n",
            "epcoh: 000246\t D_loss: 0.0065\t G_loss: 9.2274\t D_x: 0.2497\t D_g_z1: 0.0022\t D_g_z2: 0.0039\t\n",
            "epcoh: 000247\t D_loss: 0.0041\t G_loss: 8.3735\t D_x: 0.2498\t D_g_z1: 0.0016\t D_g_z2: 0.0048\t\n",
            "epcoh: 000248\t D_loss: 0.0048\t G_loss: 8.8746\t D_x: 0.2497\t D_g_z1: 0.0019\t D_g_z2: 0.0048\t\n",
            "epcoh: 000249\t D_loss: 0.0058\t G_loss: 9.6246\t D_x: 0.2496\t D_g_z1: 0.0023\t D_g_z2: 0.0084\t\n",
            "epcoh: 000250\t D_loss: 0.0035\t G_loss: 9.0488\t D_x: 0.2498\t D_g_z1: 0.0015\t D_g_z2: 0.0045\t\n",
            "epcoh: 000251\t D_loss: 0.0045\t G_loss: 9.0532\t D_x: 0.2497\t D_g_z1: 0.0013\t D_g_z2: 0.0026\t\n",
            "epcoh: 000252\t D_loss: 0.0058\t G_loss: 8.9870\t D_x: 0.2497\t D_g_z1: 0.0016\t D_g_z2: 0.0032\t\n",
            "epcoh: 000253\t D_loss: 0.0044\t G_loss: 9.6844\t D_x: 0.2499\t D_g_z1: 0.0012\t D_g_z2: 0.0062\t\n",
            "epcoh: 000254\t D_loss: 0.0043\t G_loss: 9.2141\t D_x: 0.2497\t D_g_z1: 0.0017\t D_g_z2: 0.0034\t\n",
            "epcoh: 000255\t D_loss: 0.0110\t G_loss: 8.1032\t D_x: 0.2496\t D_g_z1: 0.0023\t D_g_z2: 0.0044\t\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-8011725f1ef5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             conds_fake = encodeOneHot(lbls=torch.randint(0, 10, (BATCH_SIZE,)), \n\u001b[0;32m---> 23\u001b[0;31m                                       lbl_dim=LABEL_DIM).to(device)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             imgs_fake = generator(batch=(2 * torch.randn(BATCH_SIZE, NOISE_DIM) - 1).to(device), \n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Results\n",
        "test_z = (2 * torch.randn(10, NOISE_DIM) - 1).to(device)\n",
        "\n",
        "generated = generator(test_z, test_y).detach().cpu().view(-1, 1, 28, 28)\n",
        "\n",
        "grid = torchvision.utils.make_grid(\n",
        "    generated,\n",
        "    nrow=5,\n",
        "    padding=10,\n",
        "    pad_value=1\n",
        ")\n",
        "\n",
        "img = np.transpose(\n",
        "    grid.numpy(),\n",
        "    (1, 2, 0)\n",
        ")\n",
        "\n",
        "fig = plt.figure(figsize=(16, 16))\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(img);"
      ],
      "metadata": {
        "id": "AaKdDTMzHfmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LUq23NR1gy6F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}