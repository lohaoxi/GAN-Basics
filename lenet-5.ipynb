{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lenet-5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lohaoxi/basic-pytorch-gans/blob/master/lenet-5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdvpzJEksUo-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aabfaa66-9b85-4342-e28c-f6a3ed136992"
      },
      "source": [
        "import time\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from torchvision.datasets import MNIST\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from  torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.9.0+cu102\n",
            "0.10.0+cu102\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G4U9SJLX8Dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "723b1076-9d45-4809-b813-5d9816cd6b95"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "train_data = MNIST('./data/mnist', \n",
        "                   train = True,\n",
        "                   download = True,\n",
        "                   transform = torchvision.transforms.Compose([torchvision.transforms.Pad(2), torchvision.transforms.ToTensor() ]))\n",
        "test_data = MNIST('./data/mnist', \n",
        "                  train = False,\n",
        "                  download = True,\n",
        "                  transform = torchvision.transforms.Compose([torchvision.transforms.Pad(2),  torchvision.transforms.ToTensor()]))\n",
        "\n",
        "train_loader = DataLoader(dataset = train_data,\n",
        "                          batch_size = BATCH_SIZE,\n",
        "                          shuffle = True,\n",
        "                          num_workers = 4)\n",
        "\n",
        "test_loader = DataLoader(dataset = test_data,\n",
        "                         batch_size = BATCH_SIZE,\n",
        "                         shuffle = False,\n",
        "                         num_workers = 4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g47BlNJwokkf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4e7bdaf-a895-49e6-979d-a93b53003292"
      },
      "source": [
        "for a,b in train_loader:\n",
        "    temp = a,b\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwyH97N9owZg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cacf2e00-7285-4fa0-baaf-385a23ae1387"
      },
      "source": [
        "print(temp[0].shape)\n",
        "print(temp[1].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 1, 32, 32])\n",
            "torch.Size([32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yavaaP3hhM0m"
      },
      "source": [
        "def onehot_encoder(trg):\n",
        "    trg = trg.cpu().numpy()\n",
        "    trg_size = trg.size\n",
        "    onehot = np.zeros((trg_size, 10))\n",
        "    onehot[np.arange(trg_size), trg] = 1\n",
        "    onehot = torch.from_numpy(onehot).float()\n",
        "    return onehot\n",
        "\n",
        "# Cauchy-Schwarz Divergence\n",
        "class CSD(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CSD, self).__init__()\n",
        "\n",
        "    def forward(self, outputs, trg):\n",
        "        y = onehot_encoder(trg).to(device)\n",
        "        nominator = torch.sum(torch.mm(outputs, y.t()), dim = 1)\n",
        "        denominator = torch.norm(outputs, 2) * torch.norm(y, 2)\n",
        "        return torch.mean(-1 * torch.log(nominator / denominator))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63ODcrcAw7sd"
      },
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, \n",
        "                 conv_in_dim, \n",
        "                 conv_out_dim, \n",
        "                 conv_krn_size, \n",
        "                 conv_stride, \n",
        "                 conv_b, \n",
        "                 maxpool_krn_size, \n",
        "                 max_pool_stride):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels = conv_in_dim,\n",
        "                              out_channels = conv_out_dim,\n",
        "                              kernel_size = conv_krn_size,\n",
        "                              stride = conv_stride,\n",
        "                              bias = conv_b)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size = maxpool_krn_size,\n",
        "                                    stride = max_pool_stride)\n",
        "    def forward(self, batch, pool = True):\n",
        "        batch = self.conv(batch)\n",
        "        batch = self.relu(batch)\n",
        "        if pool == True:\n",
        "            batch = self.maxpool(batch)\n",
        "        elif pool == False:\n",
        "            batch = batch\n",
        "        return batch\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, \n",
        "                 layer_1_in_dim,\n",
        "                 layer_1_out_dim,\n",
        "                 layer_1_b,\n",
        "                 layer_2_in_dim,\n",
        "                 layer_2_out_dim,\n",
        "                 layer_2_b):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.layer_1 = nn.Linear(in_features = layer_1_in_dim,\n",
        "                                out_features = layer_1_out_dim,\n",
        "                                bias = layer_1_b)\n",
        "        self.layer_2 = nn.Linear(in_features = layer_2_in_dim,\n",
        "                                out_features = layer_2_out_dim,\n",
        "                                bias = layer_2_b)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, batch):\n",
        "        batch = self.layer_1(batch)\n",
        "        batch = self.relu(batch)\n",
        "        batch = self.layer_2(batch)\n",
        "        return batch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YydwMaKHvBoV"
      },
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv_1 = ConvBlock(conv_in_dim = 1,  \n",
        "                                conv_out_dim = 6, \n",
        "                                conv_krn_size = (5, 5), \n",
        "                                conv_stride = 1,  \n",
        "                                conv_b = True,  \n",
        "                                maxpool_krn_size = (2, 2),  \n",
        "                                max_pool_stride= 2)\n",
        "        \n",
        "        self.conv_2 = ConvBlock(conv_in_dim = 6,  \n",
        "                                conv_out_dim = 16, \n",
        "                                conv_krn_size = (5, 5), \n",
        "                                conv_stride = 1,  \n",
        "                                conv_b = True,  \n",
        "                                maxpool_krn_size = (2, 2),  \n",
        "                                max_pool_stride= 2)\n",
        "        \n",
        "        self.conv_3 = ConvBlock(conv_in_dim = 16,  \n",
        "                                conv_out_dim = 120, \n",
        "                                conv_krn_size = (5, 5), \n",
        "                                conv_stride = 1,  \n",
        "                                conv_b = True,  \n",
        "                                maxpool_krn_size = (2, 2),  \n",
        "                                max_pool_stride= 2)\n",
        "        \n",
        "        self.fc = FeedForward(layer_1_in_dim = 120,\n",
        "                              layer_1_out_dim = 84,\n",
        "                              layer_1_b = True,\n",
        "                              layer_2_in_dim = 84,\n",
        "                              layer_2_out_dim = 10,\n",
        "                              layer_2_b = True)\n",
        "\n",
        "    def forward(self, batch):\n",
        "        batch = self.conv_1(batch, pool = True)\n",
        "        batch = self.conv_2(batch, pool = True)\n",
        "        batch = self.conv_3(batch, pool = False)\n",
        "        batch = batch.view(batch.size(0), -1)\n",
        "        batch = self.fc(batch)\n",
        "        batch = F.softmax(batch)\n",
        "        return batch\n",
        "                                \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDFc-nj4HNsH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3089d2de-4b76-4496-8bd5-ba3c754be33a"
      },
      "source": [
        "model = LeNet5().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0005)\n",
        "criterion = CSD()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LeNet5(\n",
            "  (conv_1): ConvBlock(\n",
            "    (conv): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (relu): ReLU()\n",
            "    (maxpool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv_2): ConvBlock(\n",
            "    (conv): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (relu): ReLU()\n",
            "    (maxpool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv_3): ConvBlock(\n",
            "    (conv): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (relu): ReLU()\n",
            "    (maxpool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc): FeedForward(\n",
            "    (layer_1): Linear(in_features=120, out_features=84, bias=True)\n",
            "    (layer_2): Linear(in_features=84, out_features=10, bias=True)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVaUXQJDjlso",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3056cb3d-427f-43b1-d38e-3874aefa7ed9"
      },
      "source": [
        "EPOCHS = 128\n",
        "steps = len(train_loader) // BATCH_SIZE\n",
        "start_time = time.time()\n",
        "\n",
        "model.train(True)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "\n",
        "    performed_steps = 0\n",
        "    \n",
        "    for i, (src, trg) in enumerate(train_loader):\n",
        "        \n",
        "        if i == steps: break\n",
        "\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = model(src)\n",
        "        \n",
        "        loss = criterion(outputs, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss\n",
        "        \n",
        "        performed_steps += 1\n",
        "    \n",
        "    print(\"Epoch: {}, Loss: {:0.4f}, Elapsed Time: {:0.6f}\".format(epoch + 1, \n",
        "                                                                   epoch_loss / steps, \n",
        "                                                                   time.time() - start_time)\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 1.1548, Elapsed Time: 0.807044\n",
            "Epoch: 2, Loss: 1.1511, Elapsed Time: 1.437001\n",
            "Epoch: 3, Loss: 1.1524, Elapsed Time: 2.060158\n",
            "Epoch: 4, Loss: 1.1492, Elapsed Time: 2.684847\n",
            "Epoch: 5, Loss: 1.1502, Elapsed Time: 3.323942\n",
            "Epoch: 6, Loss: 1.1495, Elapsed Time: 3.940490\n",
            "Epoch: 7, Loss: 1.1497, Elapsed Time: 4.574377\n",
            "Epoch: 8, Loss: 1.1498, Elapsed Time: 5.196639\n",
            "Epoch: 9, Loss: 1.1514, Elapsed Time: 5.816620\n",
            "Epoch: 10, Loss: 1.1497, Elapsed Time: 6.450045\n",
            "Epoch: 11, Loss: 1.1506, Elapsed Time: 7.068037\n",
            "Epoch: 12, Loss: 1.1513, Elapsed Time: 7.679722\n",
            "Epoch: 13, Loss: 1.1486, Elapsed Time: 8.309180\n",
            "Epoch: 14, Loss: 1.1497, Elapsed Time: 8.939669\n",
            "Epoch: 15, Loss: 1.1499, Elapsed Time: 9.560322\n",
            "Epoch: 16, Loss: 1.1504, Elapsed Time: 10.181568\n",
            "Epoch: 17, Loss: 1.1489, Elapsed Time: 10.799386\n",
            "Epoch: 18, Loss: 1.1506, Elapsed Time: 11.419469\n",
            "Epoch: 19, Loss: 1.1490, Elapsed Time: 12.032374\n",
            "Epoch: 20, Loss: 1.1487, Elapsed Time: 12.647814\n",
            "Epoch: 21, Loss: 1.1477, Elapsed Time: 13.303295\n",
            "Epoch: 22, Loss: 1.1488, Elapsed Time: 13.933403\n",
            "Epoch: 23, Loss: 1.1501, Elapsed Time: 14.548281\n",
            "Epoch: 24, Loss: 1.1479, Elapsed Time: 15.175176\n",
            "Epoch: 25, Loss: 1.1504, Elapsed Time: 15.807094\n",
            "Epoch: 26, Loss: 1.1465, Elapsed Time: 16.442763\n",
            "Epoch: 27, Loss: 1.1508, Elapsed Time: 17.076636\n",
            "Epoch: 28, Loss: 1.1485, Elapsed Time: 17.699708\n",
            "Epoch: 29, Loss: 1.1439, Elapsed Time: 18.337610\n",
            "Epoch: 30, Loss: 1.1476, Elapsed Time: 19.010585\n",
            "Epoch: 31, Loss: 1.1503, Elapsed Time: 19.654639\n",
            "Epoch: 32, Loss: 1.1491, Elapsed Time: 20.285146\n",
            "Epoch: 33, Loss: 1.1495, Elapsed Time: 20.929774\n",
            "Epoch: 34, Loss: 1.1477, Elapsed Time: 21.555016\n",
            "Epoch: 35, Loss: 1.1467, Elapsed Time: 22.203474\n",
            "Epoch: 36, Loss: 1.1497, Elapsed Time: 22.843447\n",
            "Epoch: 37, Loss: 1.1501, Elapsed Time: 23.496706\n",
            "Epoch: 38, Loss: 1.1489, Elapsed Time: 24.138968\n",
            "Epoch: 39, Loss: 1.1492, Elapsed Time: 24.776712\n",
            "Epoch: 40, Loss: 1.1476, Elapsed Time: 25.432764\n",
            "Epoch: 41, Loss: 1.1495, Elapsed Time: 26.082243\n",
            "Epoch: 42, Loss: 1.1476, Elapsed Time: 26.717469\n",
            "Epoch: 43, Loss: 1.1481, Elapsed Time: 27.343642\n",
            "Epoch: 44, Loss: 1.1473, Elapsed Time: 27.974266\n",
            "Epoch: 45, Loss: 1.1503, Elapsed Time: 28.612619\n",
            "Epoch: 46, Loss: 1.1468, Elapsed Time: 29.247394\n",
            "Epoch: 47, Loss: 1.1472, Elapsed Time: 29.875800\n",
            "Epoch: 48, Loss: 1.1474, Elapsed Time: 30.495846\n",
            "Epoch: 49, Loss: 1.1460, Elapsed Time: 31.135962\n",
            "Epoch: 50, Loss: 1.1453, Elapsed Time: 31.756364\n",
            "Epoch: 51, Loss: 1.1489, Elapsed Time: 32.390182\n",
            "Epoch: 52, Loss: 1.1481, Elapsed Time: 33.040349\n",
            "Epoch: 53, Loss: 1.1481, Elapsed Time: 33.724533\n",
            "Epoch: 54, Loss: 1.1473, Elapsed Time: 34.359626\n",
            "Epoch: 55, Loss: 1.1469, Elapsed Time: 34.987290\n",
            "Epoch: 56, Loss: 1.1477, Elapsed Time: 35.611978\n",
            "Epoch: 57, Loss: 1.1466, Elapsed Time: 36.256409\n",
            "Epoch: 58, Loss: 1.1463, Elapsed Time: 36.914787\n",
            "Epoch: 59, Loss: 1.1493, Elapsed Time: 37.535394\n",
            "Epoch: 60, Loss: 1.1460, Elapsed Time: 38.171669\n",
            "Epoch: 61, Loss: 1.1476, Elapsed Time: 38.798114\n",
            "Epoch: 62, Loss: 1.1434, Elapsed Time: 39.412560\n",
            "Epoch: 63, Loss: 1.1491, Elapsed Time: 40.051324\n",
            "Epoch: 64, Loss: 1.1479, Elapsed Time: 40.674940\n",
            "Epoch: 65, Loss: 1.1478, Elapsed Time: 41.323960\n",
            "Epoch: 66, Loss: 1.1482, Elapsed Time: 41.955820\n",
            "Epoch: 67, Loss: 1.1487, Elapsed Time: 42.580441\n",
            "Epoch: 68, Loss: 1.1480, Elapsed Time: 43.213731\n",
            "Epoch: 69, Loss: 1.1463, Elapsed Time: 43.859974\n",
            "Epoch: 70, Loss: 1.1485, Elapsed Time: 44.479994\n",
            "Epoch: 71, Loss: 1.1473, Elapsed Time: 45.124933\n",
            "Epoch: 72, Loss: 1.1464, Elapsed Time: 45.740324\n",
            "Epoch: 73, Loss: 1.1462, Elapsed Time: 46.373666\n",
            "Epoch: 74, Loss: 1.1473, Elapsed Time: 47.024327\n",
            "Epoch: 75, Loss: 1.1458, Elapsed Time: 47.652237\n",
            "Epoch: 76, Loss: 1.1460, Elapsed Time: 48.295951\n",
            "Epoch: 77, Loss: 1.1472, Elapsed Time: 48.933574\n",
            "Epoch: 78, Loss: 1.1450, Elapsed Time: 49.549871\n",
            "Epoch: 79, Loss: 1.1493, Elapsed Time: 50.204360\n",
            "Epoch: 80, Loss: 1.1465, Elapsed Time: 50.843579\n",
            "Epoch: 81, Loss: 1.1477, Elapsed Time: 51.498963\n",
            "Epoch: 82, Loss: 1.1434, Elapsed Time: 52.143598\n",
            "Epoch: 83, Loss: 1.1458, Elapsed Time: 52.767833\n",
            "Epoch: 84, Loss: 1.1468, Elapsed Time: 53.415498\n",
            "Epoch: 85, Loss: 1.1464, Elapsed Time: 54.070964\n",
            "Epoch: 86, Loss: 1.1449, Elapsed Time: 54.710846\n",
            "Epoch: 87, Loss: 1.1487, Elapsed Time: 55.377684\n",
            "Epoch: 88, Loss: 1.1483, Elapsed Time: 56.038049\n",
            "Epoch: 89, Loss: 1.1452, Elapsed Time: 56.672138\n",
            "Epoch: 90, Loss: 1.1493, Elapsed Time: 57.324796\n",
            "Epoch: 91, Loss: 1.1456, Elapsed Time: 57.965063\n",
            "Epoch: 92, Loss: 1.1481, Elapsed Time: 58.603753\n",
            "Epoch: 93, Loss: 1.1472, Elapsed Time: 59.241092\n",
            "Epoch: 94, Loss: 1.1446, Elapsed Time: 59.879757\n",
            "Epoch: 95, Loss: 1.1460, Elapsed Time: 60.513047\n",
            "Epoch: 96, Loss: 1.1444, Elapsed Time: 61.153058\n",
            "Epoch: 97, Loss: 1.1471, Elapsed Time: 61.784298\n",
            "Epoch: 98, Loss: 1.1468, Elapsed Time: 62.421316\n",
            "Epoch: 99, Loss: 1.1469, Elapsed Time: 63.070321\n",
            "Epoch: 100, Loss: 1.1468, Elapsed Time: 63.702839\n",
            "Epoch: 101, Loss: 1.1450, Elapsed Time: 64.350029\n",
            "Epoch: 102, Loss: 1.1463, Elapsed Time: 64.979022\n",
            "Epoch: 103, Loss: 1.1461, Elapsed Time: 65.635749\n",
            "Epoch: 104, Loss: 1.1432, Elapsed Time: 66.274753\n",
            "Epoch: 105, Loss: 1.1483, Elapsed Time: 66.908829\n",
            "Epoch: 106, Loss: 1.1456, Elapsed Time: 67.538204\n",
            "Epoch: 107, Loss: 1.1467, Elapsed Time: 68.179153\n",
            "Epoch: 108, Loss: 1.1449, Elapsed Time: 68.816838\n",
            "Epoch: 109, Loss: 1.1462, Elapsed Time: 69.453867\n",
            "Epoch: 110, Loss: 1.1464, Elapsed Time: 70.085599\n",
            "Epoch: 111, Loss: 1.1467, Elapsed Time: 70.714675\n",
            "Epoch: 112, Loss: 1.1444, Elapsed Time: 71.369887\n",
            "Epoch: 113, Loss: 1.1446, Elapsed Time: 72.005954\n",
            "Epoch: 114, Loss: 1.1473, Elapsed Time: 72.663429\n",
            "Epoch: 115, Loss: 1.1463, Elapsed Time: 73.309782\n",
            "Epoch: 116, Loss: 1.1451, Elapsed Time: 73.948834\n",
            "Epoch: 117, Loss: 1.1458, Elapsed Time: 74.596362\n",
            "Epoch: 118, Loss: 1.1440, Elapsed Time: 75.220397\n",
            "Epoch: 119, Loss: 1.1495, Elapsed Time: 75.870241\n",
            "Epoch: 120, Loss: 1.1474, Elapsed Time: 76.511736\n",
            "Epoch: 121, Loss: 1.1480, Elapsed Time: 77.169786\n",
            "Epoch: 122, Loss: 1.1493, Elapsed Time: 77.799385\n",
            "Epoch: 123, Loss: 1.1465, Elapsed Time: 78.434350\n",
            "Epoch: 124, Loss: 1.1473, Elapsed Time: 79.068380\n",
            "Epoch: 125, Loss: 1.1434, Elapsed Time: 79.709641\n",
            "Epoch: 126, Loss: 1.1455, Elapsed Time: 80.345946\n",
            "Epoch: 127, Loss: 1.1464, Elapsed Time: 80.990058\n",
            "Epoch: 128, Loss: 1.1467, Elapsed Time: 81.623199\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWrvyxAJlsTH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bae80dcb-a155-4daf-dd6f-9166d68aeb54"
      },
      "source": [
        "avg_loss = 0\n",
        "avg_acc = 0\n",
        "\n",
        "model.train(False)\n",
        "\n",
        "with torch.no_grad():\n",
        "    \n",
        "    steps = 0\n",
        "\n",
        "    for src, trg in test_loader:\n",
        "        \n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "\n",
        "        outputs = model(src)\n",
        "\n",
        "        avg_loss += criterion(outputs, trg)\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        avg_acc += preds.eq(trg).sum().item()\n",
        "        \n",
        "        steps += 1\n",
        "\n",
        "print(\"Loss: {:0.2f}, Acc: {:.2%}\".format(avg_loss / steps,\n",
        "                                          avg_acc / (steps * BATCH_SIZE))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 1.15, Acc: 86.04%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}